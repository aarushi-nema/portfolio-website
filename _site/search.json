[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Aarushi Nema",
    "section": "",
    "text": "LangChain101\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models and LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Customer Support Agent Using Amazon BedRock AgentCore\n\n\n\nMachine Learning\n\nAgentic AI\n\nAWS\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\nAarushi Nema\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization: A Global Journey Through Life Expectancy\n\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\nAarushi Nema\n\n\n\n\n\n\n\n\n\n\n\n\nMindFul: Unconsious Bias\n\n\n\nApp Development\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nAarushi Nema\n\n\n\n\n\n\n\n\n\n\n\n\nRoundUp: Finance Tracking App\n\n\n\nDesign Portfolio\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nAarushi Nema\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quarto_command.html",
    "href": "quarto_command.html",
    "title": "Quarto Cache Clearing Commands",
    "section": "",
    "text": "# Stop preview\npkill -f \"quarto preview\"\n\n# Remove all cache\nrm -rf _site/ .quarto/ .quarto-cache/\n\n# Clean Quarto cache\nquarto clean\n\n# Add cache-busting comment\necho \"/* Cache bust: $(date +%s) */\" &gt;&gt; styles.css\n\n# Restart preview\nquarto preview\n\n\n\nMost Common Use Case:\npkill -f \"quarto preview\" && rm -rf _site/ .quarto/ && quarto preview\nWhen CSS Changes Don’t Show:\necho \"/* Force refresh $(date) */\" &gt;&gt; styles.css\nWhen Files Don’t Update:\nquarto clean && rm -rf _site/\n\n\n\nDeploy to GitHub Pages:\nquarto publish gh-pages --no-browser\nForce Fresh Deployment:\necho \"/* Cache bust: $(date +%s) */\" &gt;&gt; styles.css\ngit add . && git commit -m \"Force refresh\" && git push origin main\nquarto publish gh-pages --no-browser"
  },
  {
    "objectID": "quarto_command.html#complete-cache-clearing-commands",
    "href": "quarto_command.html#complete-cache-clearing-commands",
    "title": "Quarto Cache Clearing Commands",
    "section": "",
    "text": "# Stop preview\npkill -f \"quarto preview\"\n\n# Remove all cache\nrm -rf _site/ .quarto/ .quarto-cache/\n\n# Clean Quarto cache\nquarto clean\n\n# Add cache-busting comment\necho \"/* Cache bust: $(date +%s) */\" &gt;&gt; styles.css\n\n# Restart preview\nquarto preview"
  },
  {
    "objectID": "quarto_command.html#quick-reference",
    "href": "quarto_command.html#quick-reference",
    "title": "Quarto Cache Clearing Commands",
    "section": "",
    "text": "Most Common Use Case:\npkill -f \"quarto preview\" && rm -rf _site/ .quarto/ && quarto preview\nWhen CSS Changes Don’t Show:\necho \"/* Force refresh $(date) */\" &gt;&gt; styles.css\nWhen Files Don’t Update:\nquarto clean && rm -rf _site/"
  },
  {
    "objectID": "quarto_command.html#deployment-commands",
    "href": "quarto_command.html#deployment-commands",
    "title": "Quarto Cache Clearing Commands",
    "section": "",
    "text": "Deploy to GitHub Pages:\nquarto publish gh-pages --no-browser\nForce Fresh Deployment:\necho \"/* Cache bust: $(date +%s) */\" &gt;&gt; styles.css\ngit add . && git commit -m \"Force refresh\" && git push origin main\nquarto publish gh-pages --no-browser"
  },
  {
    "objectID": "index.html#tools-of-the-trade",
    "href": "index.html#tools-of-the-trade",
    "title": "Aarushi Nema",
    "section": "Tools of the Trade",
    "text": "Tools of the Trade\n\n  \n    \n      \n      Data / AI\n      I design intelligent systems combining machine learning, automated data pipelines, and responsible AI. My work includes PySpark + Airflow data pipelines at Hyundai, ETL automation and RPA at Infineon, and research on conversational recommenders (PRICAI 2025 Accepted), value-aligned LLMs (NeurIPS 2025 submission), and some Kaggle Competitions.\n      Tech stack:\n      \n        Python\n        PyTorch\n        TensorFlow\n        Scikit-Learn\n        PySpark\n        Hadoop\n        SQL\n        Tableau\n        Pandas\n      \n    \n\n    \n      \n      Web/Mobile dev\n      I build full-stack web and mobile apps that integrate APIs, databases, and responsive UIs. Examples include Flask APIs for Hadoop data retrieval, a React–Flask–SQL chatbot at Infineon, a React Native + Node.js + MongoDB app under Credit Suisse mentorship, and NTU Student Union portals modernized with ReactJS and Django.\n      Tech stack:\n      \n        Flask\n        RESTful API\n        Django\n        ReactJS\n        React Native\n        NodeJS\n        MongoDB\n        JavaScript\n        Quarto\n      \n    \n\n  \n\n  \n     \n      \n      CI/CD & Automation\n      I specialize in automating data workflows and deployment pipelines. At Infineon, I built Jenkins-based ETL automation and a UiPath + Python RPA solutions. At Hyundai, I engineered PySpark + Airflow + Bash Script pipelines to synchronize enterprise data across multiple databases.\n      Tech stack:\n      \n        UiPath\n        Jenkins\n        BitBucket\n        Git\n        Apache Airflow\n        Bash Scripting\n      \n    \n\n     \n      \n      UI/UX\n      I love to craft intuitive digital experiences by blending design and engineering. As NTUSU IT Committee Vice-Chairperson, I led Figma prototyping and redesigned portals. I also designed app interfaces under Credit Suisse mentorship and built visual dashboards with Tableau. I have also built multiple Figma based designs for websites and mobile apps for academic projects.\n      Tech stack:\n      \n        Figma\n        Shadcn\n        TailWindCSS"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Aarushi Nema",
    "section": "Experience",
    "text": "Experience\n\n  \n    \n      \n        Data Platform Intern\n        Hyundai Motor Group Innovation Centre Singapore\n      \n      \n        May 2024 – Aug 2024\n      \n    \n    \n      \n        Engineered and optimized scalable data migration pipelines from Relational DBMS (PostgreSQL and Tibero) to Hadoop Data Lake using Python, PySpark, Apache Airflow, and Bash scripting\n        Developed RESTful APIs using Flask and PySpark to enable dynamic data retrieval and filtering from Hadoop Data Lake across multiple data formats\n        Designed and implemented a dynamic data synchronization pipeline integrating multiple database systems to PostgreSQL using PySpark, ensuring data consistency and reliability\n        Created technical documentation for data pipelines\n      \n    \n  \n\n  \n    \n      \n        Software Development (Data Application) Intern\n        Infineon Technologies\n      \n      \n        May 2023 – Dec 2023\n      \n    \n    \n      \n        Collaborated closely with stakeholders and product engineers to identify data challenges and translate business requirements into robust data engineering solutions, resulting in enhanced Tableau dashboards and optimized data pipelines with new feature integrations\n        Developed Python scripts to automate extraction, transformation, and loading (ETL) process for production yield records into a SQL database using Jenkins, meeting stakeholder specifications, and creating an automated email notification system based on the data\n        Spearheaded development of a centralized Robotic Process Automation (RPA) solution from scratch utilizing UiPath and Python, resulting in an 90% reduction in human intervention across four critical software tools\n        Designed and implemented a Confluence-based chatbot using React, Flask, and SQL, significantly reducing time spent searching for and navigating through various software tools\n        Presented technical projects and dashboards to cross-functional teams, translating complex data workflows into business impact"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Aarushi Nema",
    "section": "Education",
    "text": "Education\n\n  \n    \n      \n        Nanyang Technological University, Singapore\n        Bachelor of Computing, Data Science and Artificial Intelligence (Honors with Distinction)\n      \n      \n        Aug 2021 – Jun 2025\n      \n    \n    \n      \n        Vice chairperson, NTU Student Union IT Committee (Aug 2022 - Aug 2024)\n        Hon Gen Secretary, NTU Women In Tech (Aug 2022 - Aug 2023)"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Aarushi Nema",
    "section": "Projects",
    "text": "Projects\n\n  \n    \n      \n        \n      \n      \n        AI Agent Core\n        Building a comprehensive customer support agent using Amazon BedRock AgentCore with advanced AI capabilities.\n        Read more\n      \n    \n    \n    \n      \n        \n      \n      \n        GraphRAG CRS\n        Context-aware conversational recommendation system with knowledge-grounded prompt learning and graph-based retrieval.\n        Read more\n      \n    \n\n    \n      \n        \n      \n      \n        Life Expectancy DataViz\n        Interactive data visualization exploring global life expectancy trends and patterns across different regions.\n        View project\n      \n    \n    \n    \n      \n        \n      \n      \n        MindFul App\n        Mobile application addressing unconscious bias through thoughtful design and user-centered approach.\n        View project\n      \n    \n    \n    \n      \n        \n      \n      \n        RoundUp Finance App\n        UI/UX design for a financial tracking app that helps users save money through innovative round-up features.\n        View design\n      \n  \n\n    \n      \n        \n      \n      \n        Portfolio Website\n        Personal portfolio website built with Quarto, featuring responsive design, modern aesthetics, and dynamic interactions.\n        Explore site\n      \n    \n    \n    \n      \n        More Projects →\n      \n    \n  \n\n\nLet’s Connect!\n\n  ✉️ aarushi.nema02@gmail.com\n  🔗 LinkedIn\n  🐙 GitHub"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-02-agentcore-memory.html",
    "href": "code_files/aws_agentcore/lab-02-agentcore-memory.html",
    "title": "Lab 2: Personalize our agent by adding memory",
    "section": "",
    "text": "In Lab 1, you built a Customer Support Agent that worked well for a single user in a local session. However, real-world customer support needs to scale beyond a single user running in a local environment.\nWhen we run an Agent in Production, we’ll need: - Multi-User Support: Handle thousands of customers simultaneously - Persistent Storage: Save conversations beyond session lifecycle - Long-Term Learning: Extract customer preferences and behavioral patterns - Cross-Session Continuity: Remember customers across different interactions\nWorkshop Progress: - Lab 1 (Done): Create Agent Prototype - Build a functional customer support agent - Lab 2 (Current): Enhance with Memory - Add conversation context and personalization - Lab 3: Scale with Gateway & Identity - Share tools across agents securely - Lab 4: Deploy to Production - Use AgentCore Runtime with observability - Lab 5: Build User Interface - Create a customer-facing application\nIn this lab, you’ll add the missing persistence and learning layer that transforms your Goldfish-Agent (forgets the conversation in seconds) into an smart personalized Assistant.\nMemory is a critical component of intelligence. While Large Language Models (LLMs) have impressive capabilities, they lack persistent memory across conversations. Amazon Bedrock AgentCore Memory addresses this limitation by providing a managed service that enables AI agents to maintain context over time, remember important facts, and deliver consistent, personalized experiences.\nAgentCore Memory operates on two levels: - Short-Term Memory: Immediate conversation context and session-based information that provides continuity within a single interaction or closely related sessions. - Long-Term Memory: Persistent information extracted and stored across multiple conversations, including facts, preferences, and summaries that enable personalized experiences over time."
  },
  {
    "objectID": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-3-seed-previous-customer-interactions",
    "href": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-3-seed-previous-customer-interactions",
    "title": "Lab 2: Personalize our agent by adding memory",
    "section": "Step 3: Seed previous customer interactions",
    "text": "Step 3: Seed previous customer interactions\nWhy are we seeding memory?\nIn production, agents accumulate memory naturally through customer interactions. However, for this lab, we’re seeding historical conversations to demonstrate how Long-Term Memory (LTM) works without waiting for real conversations.\nHow memory processing works: 1. create_event stores interactions in Short-Term Memory (STM) instantly 2. STM is asynchronously processed by Long-Term Memory strategies 3. LTM extracts patterns, preferences, and facts for future retrieval\nLet’s seed some customer history to see this in action:\n\n# List existing memory resources\nfor memory in memory_client.list_memories():\n    print(f\"Memory Arn: {memory.get('arn')}\")\n    print(f\"Memory ID: {memory.get('id')}\")\n    print(\"--------------------------------------------------------------------\")\n\n# Seed with previous customer interactions\nCUSTOMER_ID = \"customer_001\"\n\nprevious_interactions = [\n    (\"I'm having issues with my MacBook Pro overheating during video editing.\",\"USER\"),\n    (\"I can help with that thermal issue. For video editing workloads, let's check your Activity Monitor and adjust performance settings. Your MacBook Pro order #MB-78432 is still under warranty.\", \"ASSISTANT\"),\n    (\"What's the return policy on gaming headphones? I need low latency for competitive FPS games\", \"USER\"),\n    (\"For gaming headphones, you have 30 days to return. Since you're into competitive FPS, I'd recommend checking the audio latency specs - most gaming models have &lt;40ms latency.\", \"ASSISTANT\"),\n    (\"I need a laptop under $1200 for programming. Prefer 16GB RAM minimum and good Linux compatibility. I like ThinkPad models.\", \"USER\"),\n    (\"Perfect! For development work, I'd suggest looking at our ThinkPad E series or Dell XPS models. Both have excellent Linux support and 16GB RAM options within your budget.\", \"ASSISTANT\"),\n]\n\n# Save previous interactions\nif memory_id:\n    try:\n        memory_client.create_event(\n            memory_id=memory_id,\n            actor_id=CUSTOMER_ID,\n            session_id=\"previous_session\",\n            messages=previous_interactions\n        )\n        print(\"✅ Seeded customer history successfully\")\n        print(\"📝 Interactions saved to Short-Term Memory\")\n        print(\"⏳ Long-Term Memory processing will begin automatically...\")\n    except Exception as e:\n        print(f\"⚠️ Error seeding history: {e}\")\n\nMemory Arn: arn:aws:bedrock-agentcore:us-west-2:900569417635:memory/CustomerSupportMemory-cGl9C845Vd\nMemory ID: CustomerSupportMemory-cGl9C845Vd\n--------------------------------------------------------------------\n✅ Seeded customer history successfully\n📝 Interactions saved to Short-Term Memory\n⏳ Long-Term Memory processing will begin automatically...\n\n\n\nUnderstanding Memory Processing\nAfter creating events with create_event, AgentCore Memory processes the data in two stages:\n\nImmediate: Messages stored in Short-Term Memory (STM)\nAsynchronous: STM processed into Long-Term Memory (LTM) strategies\n\nLTM processing typically takes 20-30 seconds as the system: - Analyzes conversation patterns - Extracts customer preferences and behaviors - Creates semantic embeddings for factual information - Organizes memories by namespace for efficient retrieval\nLet’s check if our Long-Term Memory processing is complete by retrieving customer preferences:\n\nimport time\n\n# Wait for Long-Term Memory processing to complete\nprint(\"🔍 Checking for processed Long-Term Memories...\")\nretries = 0\nmax_retries = 6  # 1 minute wait\n\nwhile retries &lt; max_retries:\n    memories = memory_client.retrieve_memories(\n        memory_id=memory_id,\n        namespace=f\"support/customer/{CUSTOMER_ID}/preferences\",\n        query=\"can you summarize the support issue\"\n    )\n    \n    if memories:\n        print(f\"✅ Found {len(memories)} preference memories after {retries * 10} seconds!\")\n        break\n    \n    retries += 1\n    if retries &lt; max_retries:\n        print(f\"⏳ Still processing... waiting 10 more seconds (attempt {retries}/{max_retries})\")\n        time.sleep(10)\n    else:\n        print(\"⚠️ Memory processing is taking longer than expected. This can happen with overloading..\")\n        break\n\nprint(\"🎯 AgentCore Memory automatically extracted these customer preferences from our seeded conversations:\")\nprint(\"=\" * 80)\n\nfor i, memory in enumerate(memories, 1):\n    if isinstance(memory, dict):\n        content = memory.get('content', {})\n        if isinstance(content, dict):\n            text = content.get('text', '')\n            print(f\"  {i}. {text}\")\n\n🔍 Checking for processed Long-Term Memories...\n⏳ Still processing... waiting 10 more seconds (attempt 1/6)\n⏳ Still processing... waiting 10 more seconds (attempt 2/6)\n⏳ Still processing... waiting 10 more seconds (attempt 3/6)\n⏳ Still processing... waiting 10 more seconds (attempt 4/6)\n✅ Found 3 preference memories after 40 seconds!\n🎯 AgentCore Memory automatically extracted these customer preferences from our seeded conversations:\n================================================================================\n  1. {\"context\":\"User reported technical issue with MacBook Pro during video editing\",\"preference\":\"Uses MacBook Pro for video editing, experiencing performance/thermal challenges\",\"categories\":[\"technology\",\"computing\",\"video editing\",\"hardware\"]}\n  2. {\"context\":\"User inquired about gaming headphones with specific performance requirement\",\"preference\":\"Needs low latency gaming headphones for competitive FPS games\",\"categories\":[\"gaming\",\"audio equipment\",\"technology\"]}\n  3. {\"context\":\"User explicitly mentioned requirements for laptop purchase for programming\",\"preference\":\"Wants laptop under $1200, with 16GB RAM minimum, good Linux compatibility, preferring ThinkPad models\",\"categories\":[\"technology\",\"computing\",\"laptops\",\"programming\"]}\n\n\n\n\nExploring Semantic Memory\nSemantic memory stores factual information from conversations using vector embeddings. This enables similarity-based retrieval of relevant facts and context.\n\nimport time\n# Retrieve semantic memories (factual information)\nwhile True:\n    semantic_memories = memory_client.retrieve_memories(\n        memory_id=memory_id,\n        namespace=f\"support/customer/{CUSTOMER_ID}/semantic\",\n        query=\"information on the technical support issue\"\n    )\n    print(\"🧠 AgentCore Memory identified these factual details from conversations:\")\n    print(\"=\" * 80)\n    if memories:\n        break\n    time.sleep(10)\nfor i, memory in enumerate(semantic_memories, 1):\n    if isinstance(memory, dict):\n        content = memory.get('content', {})\n        if isinstance(content, dict):\n            text = content.get('text', '')\n            print(f\"  {i}. {text}\")\n\n🧠 AgentCore Memory identified these factual details from conversations:\n================================================================================\n  1. The user is interested in gaming headphones with low latency for competitive FPS games.\n  2. The user is looking for a laptop under $1200 for programming, with a preference for 16GB RAM and good Linux compatibility.\n  3. The user is experiencing overheating issues with their MacBook Pro during video editing."
  },
  {
    "objectID": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-3-implement-strands-hooks-to-save-and-retrieve-agent-interactions",
    "href": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-3-implement-strands-hooks-to-save-and-retrieve-agent-interactions",
    "title": "Lab 2: Personalize our agent by adding memory",
    "section": "Step 3: Implement Strands Hooks to save and retrieve agent interactions",
    "text": "Step 3: Implement Strands Hooks to save and retrieve agent interactions\nNow we’ll integrate AgentCore Memory with our agent using Strands’ hook system. This creates an automatic memory layer that works seamlessly with any agent conversation.\n\nMessageAddedEvent: Triggered when messages are added to the conversation, allowing us to retrieve and inject customer context\nAfterInvocationEvent: Fired after agent responses, enabling automatic storage of interactions to memory\n\nThe hook system ensures memory operations happen automatically without manual intervention, creating a seamless experience where customer context is preserved across conversations.\nTo create the hooks we will extend the HookProvider class:\n\nclass CustomerSupportMemoryHooks(HookProvider):\n    \"\"\"Memory hooks for customer support agent\"\"\"\n\n    def __init__(\n        self, memory_id: str, client: MemoryClient, actor_id: str, session_id: str\n    ):\n        self.memory_id = memory_id\n        self.client = client\n        self.actor_id = actor_id\n        self.session_id = session_id\n        self.namespaces = {\n            i[\"type\"]: i[\"namespaces\"][0]\n            for i in self.client.get_memory_strategies(self.memory_id)\n        }\n\n    def retrieve_customer_context(self, event: MessageAddedEvent):\n        \"\"\"Retrieve customer context before processing support query\"\"\"\n        messages = event.agent.messages\n        if (\n            messages[-1][\"role\"] == \"user\"\n            and \"toolResult\" not in messages[-1][\"content\"][0]\n        ):\n            user_query = messages[-1][\"content\"][0][\"text\"]\n\n            try:\n                all_context = []\n\n                for context_type, namespace in self.namespaces.items():\n                    # *** AGENTCORE MEMORY USAGE *** - Retrieve customer context from each namespace\n                    memories = self.client.retrieve_memories(\n                        memory_id=self.memory_id,\n                        namespace=namespace.format(actorId=self.actor_id),\n                        query=user_query,\n                        top_k=3,\n                    )\n                    # Post-processing: Format memories into context strings\n                    for memory in memories:\n                        if isinstance(memory, dict):\n                            content = memory.get(\"content\", {})\n                            if isinstance(content, dict):\n                                text = content.get(\"text\", \"\").strip()\n                                if text:\n                                    all_context.append(\n                                        f\"[{context_type.upper()}] {text}\"\n                                    )\n\n                # Inject customer context into the query\n                if all_context:\n                    context_text = \"\\n\".join(all_context)\n                    original_text = messages[-1][\"content\"][0][\"text\"]\n                    messages[-1][\"content\"][0][\n                        \"text\"\n                    ] = f\"Customer Context:\\n{context_text}\\n\\n{original_text}\"\n                    logger.info(f\"Retrieved {len(all_context)} customer context items\")\n\n            except Exception as e:\n                logger.error(f\"Failed to retrieve customer context: {e}\")\n\n    def save_support_interaction(self, event: AfterInvocationEvent):\n        \"\"\"Save customer support interaction after agent response\"\"\"\n        try:\n            messages = event.agent.messages\n            if len(messages) &gt;= 2 and messages[-1][\"role\"] == \"assistant\":\n                # Get last customer query and agent response\n                customer_query = None\n                agent_response = None\n\n                for msg in reversed(messages):\n                    if msg[\"role\"] == \"assistant\" and not agent_response:\n                        agent_response = msg[\"content\"][0][\"text\"]\n                    elif (\n                        msg[\"role\"] == \"user\"\n                        and not customer_query\n                        and \"toolResult\" not in msg[\"content\"][0]\n                    ):\n                        customer_query = msg[\"content\"][0][\"text\"]\n                        break\n\n                if customer_query and agent_response:\n                    # *** AGENTCORE MEMORY USAGE *** - Save the support interaction\n                    self.client.create_event(\n                        memory_id=self.memory_id,\n                        actor_id=self.actor_id,\n                        session_id=self.session_id,\n                        messages=[\n                            (customer_query, \"USER\"),\n                            (agent_response, \"ASSISTANT\"),\n                        ],\n                    )\n                    logger.info(\"Saved support interaction to memory\")\n\n        except Exception as e:\n            logger.error(f\"Failed to save support interaction: {e}\")\n\n    def register_hooks(self, registry: HookRegistry) -&gt; None:\n        \"\"\"Register customer support memory hooks\"\"\"\n        registry.add_callback(MessageAddedEvent, self.retrieve_customer_context)\n        registry.add_callback(AfterInvocationEvent, self.save_support_interaction)\n        logger.info(\"Customer support memory hooks registered\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-4-create-a-customer-support-agent-with-memory",
    "href": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-4-create-a-customer-support-agent-with-memory",
    "title": "Lab 2: Personalize our agent by adding memory",
    "section": "Step 4: Create a Customer Support Agent with memory",
    "text": "Step 4: Create a Customer Support Agent with memory\nNext, we will implement the Customer Support Agent just as we did in Lab 1, but this time we instantiate the class CustomerSupportMemoryHooks and we pass the memory hook to the agent contructor.\n\nimport uuid\n\nfrom strands import Agent\nfrom strands.models import BedrockModel\n\nfrom lab_helpers.lab1_strands_agent import (\n    SYSTEM_PROMPT,\n    get_return_policy, web_search,\n    get_product_info, get_technical_support, MODEL_ID\n)\n\nSESSION_ID = str(uuid.uuid4())\nmemory_hooks = CustomerSupportMemoryHooks(memory_id, memory_client, CUSTOMER_ID, SESSION_ID)\n\n\n# Initialize the Bedrock model (Anthropic Claude 3.7 Sonnet)\nmodel = BedrockModel(\n    model_id=MODEL_ID,\n    region_name=REGION\n)\n\n# Create the customer support agent with all 5 tools\nagent = Agent(\n    model=model,\n    hooks=[memory_hooks], # Pass Memory Hooks\n    tools=[\n        get_product_info,      # Tool 1: Simple product information lookup\n        get_return_policy,      # Tool 2: Simple return policy lookup\n        web_search,\n        get_technical_support\n    ],\n    system_prompt=SYSTEM_PROMPT\n)"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-5-test-personalized-agent",
    "href": "code_files/aws_agentcore/lab-02-agentcore-memory.html#step-5-test-personalized-agent",
    "title": "Lab 2: Personalize our agent by adding memory",
    "section": "Step 5: Test Personalized Agent",
    "text": "Step 5: Test Personalized Agent\nLet’s test our memory-enhanced agent! Watch how it uses the customer’s historical preferences to provide personalized recommendations.\nThe agent will automatically: 1. Retrieve relevant customer context from memory 2. Use that context to personalize the response 3. Save this new interaction for future use\n\nfrom IPython.display import display, Markdown\n\nprint(\"🎧 Testing headphone recommendation with customer memory...\\n\\n\")\nresponse1 = agent(\"Which headphones would you recommend?\")\n\n🎧 Testing headphone recommendation with customer memory...\n\n\nI'd be happy to help you find some gaming headphones with low latency, perfect for competitive FPS games where timing is critical. Let me get some specific information about gaming headphones for you.\nTool #1: get_product_info\nBased on your need for low-latency gaming headphones for competitive FPS games, let me search for some specific recommendations that would work well for your requirements:\nTool #2: web_search\nBased on the information I've gathered, here are my recommendations for gaming headphones that would be ideal for competitive FPS games where low latency is crucial:\n\n### Top Recommendations for Low Latency Gaming Headphones:\n\n1. **SteelSeries Arctis Nova Pro** - Considered a top choice for competitive gaming with excellent multi-device connectivity. The wireless version offers very low latency that's ideal for FPS games.\n\n2. **Turtle Beach Stealth 700 Gen 2** - Specifically mentioned as a next-generation competitive gaming headset with improved audio performance for better game performance.\n\n3. **Headphones with aptX Low Latency Support** - Look for headphones supporting this codec, as it can reduce latency to under 40ms, which experts consider ideal for competitive gaming.\n\n### What to Look For:\n- **Latency under 40ms** - For competitive FPS games, this is considered the benchmark for lag-free audio\n- **Good positional audio** - Critical for accurately locating enemies in FPS games\n- **Comfort** - Important for long gaming sessions\n- **Quality microphone** - For clear communication with teammates\n\n### Connection Type Consideration:\n- **Wired options** generally offer the lowest latency and are most reliable for competitive play\n- **Wireless options** with specialized gaming-focused transmission technology can also perform well\n\nWould you like more specific information about any of these models? Or would you prefer I search for options in a particular price range? I can also provide information about return policies if you'd like to try a pair before fully committing to them.\n\n\n\nprint(\"\\n💻 Testing laptop preference recall...\\n\\n\")\nresponse2 = agent(\"What is my preferred laptop brand and requirements?\")\n\n\n💻 Testing laptop preference recall...\n\n\nBased on your previous interactions, I can see your preferred laptop specifications quite clearly.\n\nYour preferred laptop requirements are:\n- Brand preference: ThinkPad models\n- Budget: Under $1200\n- RAM: Minimum of 16GB\n- Operating system compatibility: Good Linux compatibility\n- Purpose: Programming\n\nThinkPad is definitely your preferred laptop brand, and you're looking for a model that meets these specific requirements for programming work. ThinkPads are known for their excellent Linux compatibility, which aligns perfectly with your preferences.\n\nIs there anything specific about ThinkPad models you'd like to know more about, or would you like me to provide some recommendations for ThinkPad models that meet your requirements for programming? I'd be happy to search for current models that fit your budget and specifications.\n\n\nNotice how the Agent remembers: • Your gaming preferences (low latency headphones) • Your laptop preferences (ThinkPad, 16GB RAM, Linux compatibility) • Your budget constraints ($1200 for laptops) • Previous technical issues (MacBook overheating)\nThis is the power of AgentCore Memory - persistent, personalized customer experiences!"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-02-agentcore-memory.html#congratulations",
    "href": "code_files/aws_agentcore/lab-02-agentcore-memory.html#congratulations",
    "title": "Lab 2: Personalize our agent by adding memory",
    "section": "Congratulations! 🎉",
    "text": "Congratulations! 🎉\nYou have successfully completed Lab 2: Add memory to the Customer Support Agent!\n\nWhat You Accomplished:\n\nCreated a serverless managed memory with Amazon Bedrock AgentCore Memory\nImplemented long-term memory to store User-Preferences and Semantic (Factual) information.\nIntegrated AgentCore Memory with the customer support Agent using the hook mechanism provided by Strands Agents\n\n\nNext Up Lab 3 - Scaling with Gateway and Identity →"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-02-agentcore-memory.html#resources",
    "href": "code_files/aws_agentcore/lab-02-agentcore-memory.html#resources",
    "title": "Lab 2: Personalize our agent by adding memory",
    "section": "Resources",
    "text": "Resources\n\nAmazon Bedrock Agent Core Memory\nAmazon Bedrock AgentCore Memory Deep Dive blog\nStrands Agents Hooks Documentation"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-04-agentcore-runtime.html",
    "href": "code_files/aws_agentcore/lab-04-agentcore-runtime.html",
    "title": "Lab 4: Deploy to Production - Use AgentCore Runtime with Observability",
    "section": "",
    "text": "Overview\nIn Lab 3 we scaled our Customer Support Agent by centralizing tools through AgentCore Gateway with secure authentication. Now we’ll complete the production journey by deploying our agent to AgentCore Runtime with comprehensive observability. This will transform our prototype into a production-ready system that can handle real-world traffic with full monitoring and automatic scaling.\nAmazon Bedrock AgentCore Runtime is a secure, fully managed runtime that empowers organizations to deploy and scale AI agents in production, regardless of framework, protocol, or model choice. It provides enterprise-grade reliability, automatic scaling, and comprehensive monitoring capabilities.\nWorkshop Journey:\n\nLab 1 (Done): Create Agent Prototype - Built a functional customer support agent\nLab 2 (Done): Enhance with Memory - Added conversation context and personalization\nLab 3 (Done): Scale with Gateway & Identity - Shared tools across agents securely\nLab 4 (Current): Deploy to Production - Used AgentCore Runtime with observability\nLab 5: Build User Interface - Create a customer-facing application\n\n\n\nWhy AgentCore Runtime & Production Deployment Matter\nCurrent State (Lab 1-3): Agent runs locally with centralized tools but faces production challenges:\n\nAgent runs locally in a single session\nNo comprehensive monitoring or debugging capabilities\nCannot handle multiple concurrent users reliably\n\nAfter this lab, we will have a production-ready agent infrastructure with:\n\nServerless auto-scaling to handle variable demand\nComprehensive observability with traces, metrics, and logging\nEnterprise reliability with automatic error recovery\nSecure deployment with proper access controls\nEasy management through AWS console and APIs and support for real-world production workloads.\n\n\n\nAdding comprehensive observability with AgentCore Observability\nAdditionally, AgentCore Runtime integrates seamlessly with AgentCore Observability to provide full visibility into your agent’s behavior in production. AgentCore Observability automatically captures traces, metrics, and logs from your agent interactions, tool usage, and memory access patterns. In this lab we will see how AgentCore Runtime integrates with CloudWatch GenAI Observability to provide comprehensive monitoring and debugging capabilities.\nFor request tracing, AgentCore Observability captures the complete conversation flow including tool invocations, memory retrievals, and model interactions. For performance monitoring, it tracks response times, success rates, and resource utilization to help optimize your agent’s performance.\nDuring the observability flow, AgentCore Runtime automatically instruments your agent code and sends telemetry data to CloudWatch. You can then use CloudWatch dashboards and GenAI Observability features to analyze patterns, identify bottlenecks, and troubleshoot issues in real-time.\n\n\nArchitecture for Lab 4\n\n&lt;img src=\"images/architecture_lab4_runtime.png\" width=\"75%\"/&gt; \n\nAgent now runs in AgentCore Runtime with full observability through CloudWatch, serving production traffic with auto-scaling and comprehensive monitoring. Memory and Gateway integrations from previous labs remain fully functional in the production environment.\n\n\nKey Features\n\nServerless Agent Deployment: Transform your local agent into a scalable production service using AgentCore Runtime with minimal code changes\nComprehensive Observability: Full request tracing, performance metrics, and debugging capabilities through CloudWatch GenAI Observability\n\n\n\nPrerequisites\n\nPython 3.12+\nAWS account with appropriate permissions\nDocker, Finch or Podman installed and running\nAmazon Bedrock AgentCore SDK\nStrands Agents framework\n\nNote: You MUST enable CloudWatch Transaction Search to be able to see AgentCore Observability traces in CloudWatch.\n\n\nStep 1: Import Required Libraries\n\n# Import required libraries\nimport os\nimport json\nimport boto3\nfrom strands import Agent\nfrom strands.models import BedrockModel\nfrom lab_helpers.lab2_memory import create_or_get_memory_resource\n\ncreate_or_get_memory_resource()  # Just in case the memory lab wasn't executed\n\n\n\nStep 2: Preparing Your Agent for AgentCore Runtime\n\nCreating the Runtime-Ready Agent\nLet’s first define the necessary AgentCore Runtime components via Python SDK within our previous local agent implementation.\nObserve the #### AGENTCORE RUNTIME - LINE i #### comments below to see where is the relevant deployment code added. You’ll find 4 such lines that prepare the runtime-ready agent:\n\nImport the Runtime App with from bedrock_agentcore.runtime import BedrockAgentCoreApp\nInitialize the App with app = BedrockAgentCoreApp()\nDecorate our invocation function with @app.entrypoint\nLet AgentCore Runtime control the execution with app.run()\n\n\n%%writefile ./lab_helpers/lab4_runtime.py\nfrom bedrock_agentcore.runtime import (\n    BedrockAgentCoreApp,\n)  #### AGENTCORE RUNTIME - LINE 1 ####\nfrom strands import Agent\nfrom strands.models import BedrockModel\nfrom scripts.utils import get_ssm_parameter\nfrom lab_helpers.lab1_strands_agent import (\n    get_return_policy,\n    get_product_info,\n    get_technical_support,\n    SYSTEM_PROMPT,\n    MODEL_ID,\n)\n\nfrom lab_helpers.lab2_memory import (\n    CustomerSupportMemoryHooks,\n    memory_client,\n    ACTOR_ID,\n    SESSION_ID,\n)\n\n# Lab1 import: Create the Bedrock model\nmodel = BedrockModel(model_id=MODEL_ID)\n\n# Lab2 import : Initialize memory via hooks\nmemory_id = get_ssm_parameter(\"/app/customersupport/agentcore/memory_id\")\nmemory_hooks = CustomerSupportMemoryHooks(\n    memory_id, memory_client, ACTOR_ID, SESSION_ID\n)\n\n# Create the agent with all customer support tools\nagent = Agent(\n    model=model,\n    tools=[get_return_policy, get_product_info, get_technical_support],\n    system_prompt=SYSTEM_PROMPT,\n    hooks=[memory_hooks],\n)\n\n# Initialize the AgentCore Runtime App\napp = BedrockAgentCoreApp()  #### AGENTCORE RUNTIME - LINE 2 ####\n\n\n@app.entrypoint  #### AGENTCORE RUNTIME - LINE 3 ####\ndef invoke(payload):\n    \"\"\"AgentCore Runtime entrypoint function\"\"\"\n    user_input = payload.get(\"prompt\", \"\")\n\n    # Invoke the agent\n    response = agent(user_input)\n    return response.message[\"content\"][0][\"text\"]\n\n\nif __name__ == \"__main__\":\n    app.run()  #### AGENTCORE RUNTIME - LINE 4 ####\n\n\n\nWhat happens behind the scenes?\nWhen you use BedrockAgentCoreApp, it automatically:\n\nCreates an HTTP server that listens on port 8080\nImplements the required /invocations endpoint for processing requests\nImplements the /ping endpoint for health checks\nHandles proper content types and response formats\nManages error handling according to AWS standards\n\n\n\n\nStep 3: Deploying to AgentCore Runtime\nNow let’s deploy our agent to AgentCore Runtime using the AgentCore Starter Toolkit.\n\nConfigure the Secure Runtime Deployment (AgentCore Runtime + AgentCore Identity)\nFirst we will use our starter toolkit to configure the AgentCore Runtime deployment with an entrypoint, the execution role we will create and a requirements file. We will also configure the identity authorization using an Amazon Cognito user pool and we will configure the starter kit to auto create the Amazon ECR repository on launch.\nDuring the configure step, your docker file will be generated based on your application code\n\n&lt;img src=\"images/configure.png\" width=\"75%\"/&gt; \n\nNote: The Cognito access_token is valid for 2 hours only. If the access_token expires you can vend another access_token by using the reauthenticate_user method.\n\nfrom lab_helpers.utils import setup_cognito_user_pool, reauthenticate_user\n\nprint(\"Setting up Amazon Cognito user pool...\")\ncognito_config = (\n    setup_cognito_user_pool()\n)  # You'll get your bearer token from this output cell.\nprint(\"Cognito setup completed ✓\")\n\n\nfrom bedrock_agentcore_starter_toolkit import Runtime\nfrom lab_helpers.utils import create_agentcore_runtime_execution_role\n\n# Initialize the runtime toolkit\nboto_session = boto3.session.Session()\nregion = boto_session.region_name\n\nexecution_role_arn = create_agentcore_runtime_execution_role()\n\nagentcore_runtime = Runtime()\n\n# Configure the deployment\nresponse = agentcore_runtime.configure(\n    entrypoint=\"lab_helpers/lab4_runtime.py\",\n    execution_role=execution_role_arn,\n    auto_create_ecr=True,\n    requirements_file=\"requirements.txt\",\n    region=region,\n    agent_name=\"customer_support_agent\",\n    authorizer_configuration={\n        \"customJWTAuthorizer\": {\n            \"allowedClients\": [cognito_config.get(\"client_id\")],\n            \"discoveryUrl\": cognito_config.get(\"discovery_url\"),\n        }\n    },\n)\n\nprint(\"Configuration completed:\", response)\n\n\n\nLaunch the Agent\nNow let’s launch our agent to AgentCore Runtime. This will create an AWS CodeBuild pipeline, the Amazon ECR repository and the AgentCore Runtime components.\n\n&lt;img src=\"images/launch.png\" width=\"100%\"/&gt; \n\n\n# Launch the agent (this will build and deploy the container)\nfrom lab_helpers.utils import put_ssm_parameter\n\nlaunch_result = agentcore_runtime.launch()\nprint(\"Launch completed:\", launch_result.agent_arn)\n\nagent_arn = put_ssm_parameter(\n    \"/app/customersupport/agentcore/runtime_arn\", launch_result.agent_arn\n)\n\n\n\nCheck Deployment Status\nLet’s wait for the deployment to complete:\n\nimport time\n\n# Wait for the agent to be ready\nstatus_response = agentcore_runtime.status()\nstatus = status_response.endpoint[\"status\"]\n\nend_status = [\"READY\", \"CREATE_FAILED\", \"DELETE_FAILED\", \"UPDATE_FAILED\"]\nwhile status not in end_status:\n    print(f\"Waiting for deployment... Current status: {status}\")\n    time.sleep(10)\n    status_response = agentcore_runtime.status()\n    status = status_response.endpoint[\"status\"]\n\nprint(f\"Final status: {status}\")\n\n\n\n\nStep 4: Invoking Your Deployed Agent\nNow that our agent is deployed and ready, let’s test it with some queries. We invoke the agent with the right authorization token type. In out case it’ll be Cognito access token. Copy the access token from the cell above\n\n&lt;img src=\"images/invoke.png\" width=\"100%\"/&gt; \n\n\nUsing the AgentCore Starter Toolkit\nWe can validate that the agent works using the AgentCore Starter Toolkit for invocation. The starter toolkit can automatically create a session id for us to query our agent. Alternatively, you can also pass the session id as a parameter during invocation. For demonstration purpose, we will create our own session id.\n\nimport uuid\n\n# Create a session ID for demonstrating session continuity\nsession_id = uuid.uuid4()\n\n# Test different customer support scenarios\nuser_query = \"My Iphone is not connecting with the Bluetooth. What should I do?\"\n\nbearer_token = reauthenticate_user(\n    cognito_config.get(\"client_id\"), \n    cognito_config.get(\"client_secret\")\n)\n\nresponse = agentcore_runtime.invoke(\n    {\"prompt\": user_query}, \n    bearer_token=bearer_token,\n    session_id=str(session_id)\n)\nresponse\n\n\n\nInvoking the agent with session continuity\nSince we are using AgentCore Runtime, we can easily continue our conversation with the same session id.\n\nuser_query = \"I've turned my Bluetooth on and off but it still does not work\"\nresponse = agentcore_runtime.invoke(\n    {\"prompt\": user_query}, \n    bearer_token=bearer_token,\n    session_id=str(session_id)\n)\nresponse\n\n\n\nInvoking the agent with a new user\nIn the example below we have not mentioned the Iphone device in the second query, but our agent still has the context of it. This is due to the AgentCore Runtime session continuity. The agent won’t know the context for a new user.\n\n# Creating a new session ID for demonstrating new customer\nsession_id2 = uuid.uuid4()\n\nuser_query = \"Still not working. What is going on?\"\nresponse = agentcore_runtime.invoke(\n    {\"prompt\": user_query}, \n    bearer_token=bearer_token,\n    session_id=str(session_id2)\n)\nresponse\n\nIn this case our agent does not have the context anymore and needs more information.\nAnd it is all it takes to have a secure and scalable endpoint for our Agent with no need to manage all the underlying infrastructure!\n\n\n\nStep 5: AgentCore Observability\nAgentCore Observability provides monitoring and tracing capabilities for AI agents using Amazon OpenTelemetry Python Instrumentation and Amazon CloudWatch GenAI Observability.\n\nAgents\nDefault AgentCore Runtime configuration allows for logging our agent’s traces on CloudWatch by means of AgentCore Observability. These traces can be seen on the AWS CloudWatch GenAI Observability dashboard. Navigate to Cloudwatch → GenAI Observability → Bedrock AgentCore.\n\n\n\nAgents Overview on CloudWatch\n\n\n\n\nSessions\nThe Sessions view shows the list of all the sessions associated with all agents in your account.\n\n\n\nsessions\n\n\n\n\nTraces\nTrace view lists all traces from your agents in this account. To work with traces:\n\nChoose Filter traces to search for specific traces.\nSort by column name to organize results.\nUnder Actions, select Logs Insights to refine your search by querying across your log and span data or select Export selected traces to export.\n\n\n\n\ntraces\n\n\n\n\n\nCongratulations! 🎉\nYou have successfully completed Lab 4: Deploy to Production - Use AgentCore Runtime with Observability!\nHere is what you accomplished:\n\nProduction-Ready Deployment:\n\nPrepared your agent for production with minimal code changes (only 4 lines added)\nValidated proper session isolation between different customers\nConfirmed session continuity + memory persistence and context awareness per session\n\n\n\nEnterprise-Grade Security & Identity:\n\nImplemented secure authentication using Cognito integration with JWT tokens\nConfigured proper IAM roles and execution permissions for production workloads\nEstablished identity-based access control for secure agent invocation\n\n\n\nComprehensive Observability:\n\nEnabled AgentCore Observability for full request tracing across all customer sessions\nConfigured CloudWatch GenAI Observability dashboard monitoring\n\n\n\nCurrent Limitations (We’ll fix these next!):\n\nDeveloper Focused Interaction - Agent accessible via SDK/API calls but no user-friendly web interface\nManual Session Management - Requires programmatic session creation rather than intuitive user experience\n\n\n\nNext Up Lab 5: Build User Interface →\nIn Lab 5, you’ll complete the customer experience by building a user-friendly interface !! Lets go !!"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-01-create-an-agent.html",
    "href": "code_files/aws_agentcore/lab-01-create-an-agent.html",
    "title": "Lab 1: Creating a simple customer support agent prototype",
    "section": "",
    "text": "Amazon Bedrock AgentCore helps you deploying and operating AI agents securely at scale - using any framework and model. It provides you with the capability to move from prototype to production faster.\nIn this 5-labs tutorial, we will demonstrate the end-to-end journey from prototype to production using a Customer Support Agent. For this example we will use Strands Agents, a simple-to-use, code-first framework for building agents and the Anthropic Claude Sonnet 3.7 model from Amazon Bedrock. For your application you can use the framework and model of your choice. It’s important to note that the concepts covered here can be applied using other frameworks and models as well.\nWorkshop Journey: - Lab 1 (Current): Create Agent Prototype - Build a functional customer support agent - Lab 2: Enhance with Memory - Add conversation context and personalization - Lab 3: Scale with Gateway & Identity - Share tools across agents securely - Lab 4: Deploy to Production - Use AgentCore Runtime with observability - Lab 5: Build User Interface - Create a customer-facing application\nIn this first lab, we’ll build a Customer Support Agent prototype that will evolve throughout the workshop into a production-ready system serving multiple customers with persistent memory, shared tools, and full observability. Our agent will have the following local tools available: - get_return_policy() - Get return policy for specific products - get_product_info() - Get product information - web_search() - Search the web for troubleshooting help - get_technical_support() - Search a Bedrock Knowledge Base"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-01-create-an-agent.html#lab-1-complete",
    "href": "code_files/aws_agentcore/lab-01-create-an-agent.html#lab-1-complete",
    "title": "Lab 1: Creating a simple customer support agent prototype",
    "section": "🎉 Lab 1 Complete!",
    "text": "🎉 Lab 1 Complete!\nYou’ve successfully created a functional Customer Support Agent prototype! Here’s what you accomplished:\n\nBuilt an agent with 3 custom tools (return policy, product info, web search)\n\nTested multi-tool interactions and web search capabilities\n\nEstablished the foundation for our production journey\n\n\nCurrent Limitations (We’ll fix these!)\n\nSingle user conversation memory - local conversation session, multiple customers need multiple sessions.\nConversation history limited to session - no long term memory or cross session information is available in the conversation.\nTools reusability - tools aren’t reusable across different agents\n\nRunning locally only - not scalable\nIdentity - No user and/or agent identity or access control\nObservability - Limited observability into agent behavior\nExisting APIs - No access to existing enterprise APIs for customer data\n\n\nNext Up Lab 2: Personalize our agent by adding memory →"
  },
  {
    "objectID": "posts/machine_learning_projects/Longetivity_dataviz.html",
    "href": "posts/machine_learning_projects/Longetivity_dataviz.html",
    "title": "Data Visualization: A Global Journey Through Life Expectancy",
    "section": "",
    "text": "This project explores global life expectancy trends through interactive data visualizations."
  },
  {
    "objectID": "posts/learning/activeloop_course/01_langchain101.html",
    "href": "posts/learning/activeloop_course/01_langchain101.html",
    "title": "LangChain101",
    "section": "",
    "text": "Fundamental concept of LangChain Revolves around invoking an LLM for a specific input.\n# Import the LLM Wrapper\nfrom langchain.llms import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue"
  },
  {
    "objectID": "posts/learning/activeloop_course/01_langchain101.html#the-llm",
    "href": "posts/learning/activeloop_course/01_langchain101.html#the-llm",
    "title": "LangChain101",
    "section": "The LLM",
    "text": "The LLM\nTemperature in an LLM model is a measure of randomness in the output. It ranges from 0 to 1, with 0 for more stability and probable results and 1 for more inconsistent but interesting result. For creative tasks, a temperature between 0.70 and 0.90 offers a balance of reliability and creativity.\n\n# gpt-3.5-turbo-instruct - for plain text continuation via the /v1/completions endpoint. Best for single-shot tasks, text continuation\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n\n\ntext = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\nprint(llm(text))\n\n\n\nMonday:\nWarm up: 10 minute jog\nCircuit 1:\n- 20 jumping jacks\n- 10 push-ups\n- 20 mountain climbers\n- 10 burpees\n- 20 high knees\n- 20 bicycle crunches\nRepeat circuit 3 times with minimal rest in between exercises.\n\nTuesday:\n30 minute outdoor run or bike ride. Alternate between 3 minutes at a moderate pace and 1 minute at a faster pace for a total of 30 minutes.\n\nWednesday:\nWarm up: 5 minute brisk walk\nCircuit 2:\n- 20 step-ups (use a bench or stairs)\n- 10 tricep dips (use a park bench)\n- 20 squats\n- 10 lunges (each leg)\n- 20 Russian twists (use a water bottle or small weight)\n- 1 minute plank\nRepeat circuit 3 times with minimal rest in between exercises.\n\nThursday:\n45 minute hike or nature walk with some incline intervals. Alternate between walking at a steady pace and increasing the incline for short bursts.\n\nFriday:\nWarm up: 5 minute jog\nCircuit 3:\n- 20 jumping lunges\n- 10 push-ups with side plank rotation (5 each side)\n-"
  },
  {
    "objectID": "posts/learning/activeloop_course/01_langchain101.html#the-chains",
    "href": "posts/learning/activeloop_course/01_langchain101.html#the-chains",
    "title": "LangChain101",
    "section": "The Chains",
    "text": "The Chains\nIn LangChain, a chain is a wrapper around multiple invidividual components. So we use a chain to combine multiple components in a specific sequence. There are multiple types of chains and the most common one is the LLMChain, which consists of: (1) PromptTemplate (2) a model (LLM/ChatModel), (3) an optiona output parser\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\"\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nprint(chain.run(\"eco-friendly water bottle\"))\n\n\n\nGreenHydro Bottles"
  },
  {
    "objectID": "posts/learning/activeloop_course/01_langchain101.html#the-memory",
    "href": "posts/learning/activeloop_course/01_langchain101.html#the-memory",
    "title": "LangChain101",
    "section": "The Memory",
    "text": "The Memory\nIn LangChain, the memory is how we store and manage the conversation history between the user and the AI. It keeps the context and cohenrency throughout the interaction.\nTypes of chains in LangChain: (1) LLMChain = stateless, single prompt → response. (2) ConversationChain = stateful, keeps history in memory and reuses it across turns.\n\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory # acts as a buffer to store the conversation history\n\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\nconversation = ConversationChain(\n    llm=llm,\n    verbose=True, # will print out extra logs about what’s happening inside the chain\n    memory=ConversationBufferMemory()\n)\n\nconversation.predict(input=\"Tell me about yourself.\")\nconversation.predict(input=\"What can you do?\")\nconversation.predict(input=\"How can you help me with data analysis?\")\n\n# print(conversation)\n\n\n\n\n&gt; Entering new ConversationChain chain...\n\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n\n\nCurrent conversation:\n\n\n\nHuman: Tell me about yourself.\n\nAI:\n\n\n\n&gt; Finished chain.\n\n\n\n\n\n&gt; Entering new ConversationChain chain...\n\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n\n\nCurrent conversation:\n\nHuman: Tell me about yourself.\n\nAI:  Well, I am an artificial intelligence program designed and created by a team of programmers. I am constantly learning and improving my abilities through algorithms and data analysis. My main purpose is to assist and provide information to users like yourself. I am currently housed in a server located in a data center, but I can also be accessed through various devices such as computers, smartphones, and smart speakers. Is there anything specific you would like to know about me?\n\nHuman: What can you do?\n\nAI:\n\n\n\n&gt; Finished chain.\n\n\n\n\n\n&gt; Entering new ConversationChain chain...\n\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n\n\nCurrent conversation:\n\nHuman: Tell me about yourself.\n\nAI:  Well, I am an artificial intelligence program designed and created by a team of programmers. I am constantly learning and improving my abilities through algorithms and data analysis. My main purpose is to assist and provide information to users like yourself. I am currently housed in a server located in a data center, but I can also be accessed through various devices such as computers, smartphones, and smart speakers. Is there anything specific you would like to know about me?\n\nHuman: What can you do?\n\nAI:  I have a wide range of capabilities, but some of my main functions include answering questions, performing tasks, and providing recommendations based on data analysis. I can also understand and respond to natural language, making it easier for users to communicate with me. Additionally, I am constantly learning and updating my knowledge base, so my abilities are constantly expanding. Is there something specific you would like me to do for you?\n\nHuman: How can you help me with data analysis?\n\nAI:\n\n\n\n&gt; Finished chain.\n\n\n\n\n' I can assist with data analysis by quickly sorting through large amounts of data and identifying patterns and trends. I can also provide visualizations and reports to help you better understand the data. Additionally, I can make predictions and recommendations based on the data, which can be useful for decision making. Is there a specific dataset or analysis you would like me to help with?'"
  },
  {
    "objectID": "posts/learning/activeloop_course/01_langchain101.html#deep-lake-vectorstore",
    "href": "posts/learning/activeloop_course/01_langchain101.html#deep-lake-vectorstore",
    "title": "LangChain101",
    "section": "Deep Lake VectorStore",
    "text": "Deep Lake VectorStore\nDeep Lake provides storage for embeddings and their corresponding metadata in the context of LLM Apps. It allows us to do searches and data retrieval on these embeddings and their. It also integrates with LangChain.\nDeepLake is multimodal (allows storage of different types of files + their vectore representations such as text, images, audio, video etc.) It is serverless so we can create and manage cloud datasets. For easy data loading out of the datalake, DeepLake has a data loader.\n\nfrom dotenv import load_dotenv\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import DeepLake\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\n\nload_dotenv()\n\nTrue\n\n\n\n# instantiate the LLM and embeddings models\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\nembeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n\n# create our docs\ntexts = [\n    \"Napoleon Bonaparte was born in 15 August 1769\",\n    \"Louis XIV was born in 5 September 1638\"\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.create_documents(texts)\n\n# create Deep Lake dataset\nmy_activeloop_org_id = \"aarushinema\" \nmy_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\ndataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\ndb = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n\n# add documents to our Deep Lake dataset\ndb.add_documents(docs)\n\n/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/2361674661.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n  llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/2361674661.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/2361674661.py:18: LangChainDeprecationWarning: This class is deprecated and will be removed in a future version. You can swap to using the `DeeplakeVectorStore` implementation in `langchain-deeplake`. Please do not submit further PRs to this class.See &lt;https://github.com/activeloopai/langchain-deeplake&gt;\n  db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\nUsing embedding function is deprecated and will be removed in the future. Please use embedding instead.\n\n\nYour Deep Lake dataset has been successfully created!\n\n\nCreating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:23&lt;00:00, 23.45s/it]\n\n\nDataset(path='hub://aarushinema/langchain_course_from_zero_to_hero', tensors=['text', 'metadata', 'embedding', 'id'])\n\n  tensor      htype      shape     dtype  compression\n  -------    -------    -------   -------  ------- \n   text       text      (2, 1)      str     None   \n metadata     json      (2, 1)      str     None   \n embedding  embedding  (2, 1536)  float32   None   \n    id        text      (2, 1)      str     None   \n\n\n\n\n\n['e03babf2-977f-11f0-9104-5e86525bdce2',\n 'e03badaa-977f-11f0-9104-5e86525bdce2']\n\n\n\n# retrivalqa chain\nretrieval_qa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=db.as_retriever()\n)\n\n\n# create an agent that uses RetrivalQA chain as a tool\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\n\ntools = [\n    Tool(\n        name=\"Retrieval QA System\",\n        func=retrieval_qa.run,\n        description=\"Use this to answer questions\"\n    )\n]\n\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\n/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/209183474.py:13: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation &lt;https://langchain-ai.github.io/langgraph/&gt;`_ as well as guides for `Migrating from AgentExecutor &lt;https://python.langchain.com/docs/how_to/migrate_agent/&gt;`_ and LangGraph's `Pre-built ReAct agent &lt;https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/&gt;`_.\n  agent = initialize_agent(\n\n\n\nresponse = agent.run(\"When was Napoleone born?\")\nprint(response)\n\n/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/1676272850.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  response = agent.run(\"When was Napoleone born?\")\n\n\n\n\n&gt; Entering new AgentExecutor chain...\n\n I should use the Retrieval QA System to answer this question\n\nAction: Retrieval QA System\n\nAction Input: \"When was Napoleone born?\"\n\nObservation: \n\nNapoleon Bonaparte was born in 15 August 1769.\n\nThought: I now know the final answer\n\nFinal Answer: Napoleon Bonaparte was born in 15 August 1769.\n\n\n\n&gt; Finished chain.\n\nNapoleon Bonaparte was born in 15 August 1769.\n\n\n\n\n\n# reloading an existing vector store and adding more data\n# load the existing Deep Lake dataset and specify the embedding function\ndb = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n\n# create new documents\ntexts = [\n    \"Lady Gaga was born in 28 March 1986\",\n    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.create_documents(texts)\n\n# add documents to our Deep Lake dataset\ndb.add_documents(docs)\n\nUsing embedding function is deprecated and will be removed in the future. Please use embedding instead.\n\n\nDeep Lake Dataset in hub://aarushinema/langchain_course_from_zero_to_hero already exists, loading from the storage\n\n\nCreating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:24&lt;00:00, 24.86s/it]\n\n\nDataset(path='hub://aarushinema/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n\n  tensor      htype      shape     dtype  compression\n  -------    -------    -------   -------  ------- \n embedding  embedding  (4, 1536)  float32   None   \n    id        text      (4, 1)      str     None   \n metadata     json      (4, 1)      str     None   \n   text       text      (4, 1)      str     None   \n\n\n\n\n\n['58fbf5dc-9781-11f0-9104-5e86525bdce2',\n '58fbf73a-9781-11f0-9104-5e86525bdce2']\n\n\n\n# instantiate the wrapper class for GPT3\nllm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n\n# create a retreiver from the db\nretrieval_qa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=db.as_retriever()\n)\n\n# instantiate tool that uses the retreiver\ntools = [\n    Tool(\n        name=\"Retrieval QA System\",\n        func=retrieval_qa.run,\n        description=\"Use this to answer questions\"\n    )\n]\n\n# create an agent that uses the tool\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\n\nresponse = agent.run(\"When was Michael Jordan born?\")\nprint(response)\n\n\n\n&gt; Entering new AgentExecutor chain...\n\n I should use a retrieval QA system to answer this question\n\nAction: Retrieval QA System\n\nAction Input: \"When was Michael Jordan born?\"\n\nObservation:  Michael Jordan was born on 17 February 1963.\n\nThought: I now know the final answer\n\nFinal Answer: Michael Jordan was born on 17 February 1963.\n\n\n\n&gt; Finished chain.\n\nMichael Jordan was born on 17 February 1963."
  },
  {
    "objectID": "posts/learning/activeloop_course/01_langchain101.html#agents-in-langchain",
    "href": "posts/learning/activeloop_course/01_langchain101.html#agents-in-langchain",
    "title": "LangChain101",
    "section": "Agents in LangChain",
    "text": "Agents in LangChain\nIn Langchain, agents are high-level components that use language models (LLMs) to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. There are different types of agents in LangChain: 1. The zero-shot-react-description agent uses the ReAct framework to decide which tool to employ based purely on the tool’s description. It necessitates a description of each tool. 2. The react-docstore agent engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document. 3. The self-ask-with-search agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool. 4. The conversational-react-description agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions.\n\nfrom langchain_core.tools import Tool\nfrom langchain_google_community import GoogleSearchAPIWrapper\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nTrue\n\n\n\nsearch = GoogleSearchAPIWrapper()\n\ntools = Tool(\n        name=\"Google Search\",\n        description=\"Use this to search the web\",\n        func=search.run\n    )\n\n\ntools.run(\"Obama's first name?\")\n\n'A member of the Democratic Party, he was the first African American president. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008\\xa0... Child\\'s First Name (Type or print) lb. Middle Name. BARACK. HUSSEIN. CERTIFICATE OF LIVE BIRTH. FILE 151. NUMBER le. DEPARTMENT OF HEALTH. 61. 10641. Last Name. Jan 28, 2021 ... Obama\\'s name (particularly his middle name Hussein) was the object of xenophobic innuendo questioning his loyalty (especially in the context of\\xa0... Apr 12, 2017 ... Why is Barack Obama\\'s full name \"Barack Hussein Obama\"? It\\'s what his parents named him … President Barack Obama, First Lady Michelle Obama, and their daughters, Malia, left, and Sasha, right, sit for a family portrait in the Oval Office,\\xa0... Barack Obama ; Barack Hussein Obama II. (1961-08-04) August 4, 1961 (age 64) Honolulu, Hawaii, U.S. · Democratic · Michelle Robinson. \\u200b. ( m. · 1992)\\u200b · Malia\\xa0... Barack Hussein Obama II was born August 4, 1961, in Honolulu, Hawaii, to parents Barack H. Obama, Sr., and Stanley Ann Dunham. Obama Barack H. Obama. Photo: Pete Souza, Obama-Biden Transition Project, licensed by Attribution Share Alike 3.0. Full name: Barack Hussein Obama Born: 4\\xa0... it specifically asks for \"Full former name(s). Obama put “None”, when in fact he went by the name Barry Soetoro, and Barry Obama. It is further believed\\xa0... Aug 16, 2024 ... ... Obama, the former president and first lady, endorsed her, the campaign promoted the video as, “The Obamas call Kamala.” And when Harris and\\xa0...'\n\n\nWe can use the “k” parameter to set number of results\n\nsearch = GoogleSearchAPIWrapper(k=1)\n\ntool = Tool(\n    name=\"I'm feeling lucky\",\n    description=\"Search Google and return the first result\",\n    func=search.run\n)\n\n\ntool.run(\"python\")\n\n'Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More'\n\n\n\nsearch = GoogleSearchAPIWrapper()\n\n\ndef top5_results(query):\n    return search.results(query, 5)\n\n\ntool = Tool(\n    name=\"Google Search Snippets\",\n    description=\"Search Google for recent results.\",\n    func=top5_results,\n)"
  },
  {
    "objectID": "posts/learning/activeloop_course/02_LLMs.html",
    "href": "posts/learning/activeloop_course/02_LLMs.html",
    "title": "Large Language Models and LangChain",
    "section": "",
    "text": "LLMs: - learn token distribution and predict the next token - are deep learning models with billions of parameters - excel at NLP tasks - can be used without “finetuning” but instead by employing “prompting” (prompt -&gt; question with examples of similar problems and solutions)\nLLM Architecture: multiple layers of a neural networks, feedforward layers, embedding layers, and attention layers."
  },
  {
    "objectID": "posts/learning/activeloop_course/02_LLMs.html#maximum-number-of-tokens",
    "href": "posts/learning/activeloop_course/02_LLMs.html#maximum-number-of-tokens",
    "title": "Large Language Models and LangChain",
    "section": "Maximum number of tokens",
    "text": "Maximum number of tokens\nIn LangChain Library, the LLM context size, or the maximum number of tokens the model can process. It is determined by the specific implementation of the LLM.\nTo find the maximum number of tokens for the OpenAI model, refer to the max_tokensCopy attribute. For example, if you’re using the GPT-3Copy model, the maximum number of tokens supported by the model is 2,049.\nIt is important to ensure that the input text does not exceed the maximum number of tokens supported by the model, as this may result in truncation or errors during processing. To handle this, you can split the input text into smaller chunks and process them separately, making sure that each chunk is within the allowed token limit. You can then combine the results as needed."
  },
  {
    "objectID": "posts/design_portfolio/round_up.html",
    "href": "posts/design_portfolio/round_up.html",
    "title": "RoundUp: Finance Tracking App",
    "section": "",
    "text": "This project was part of my university course “Venturing into Entrepreneurship.” This was a group project so shout out to my wonderful team members: Prajwal Jagadeesh Kori, Subramania Suresh Sabarish, Phang Jin Jiat Matthias, and Mahir Murtaza, without whom this project wouldn’t have been such a wonderful success! My main role in the project was UI/UX design and technical feasibility analysis."
  },
  {
    "objectID": "posts/design_portfolio/round_up.html#what-is-it",
    "href": "posts/design_portfolio/round_up.html#what-is-it",
    "title": "RoundUp: Finance Tracking App",
    "section": "What is it?",
    "text": "What is it?\nRoundUp is a mobile app to help users save and invest money without having to reserve large amounts of their salaries."
  },
  {
    "objectID": "posts/design_portfolio/round_up.html#how-does-it-work",
    "href": "posts/design_portfolio/round_up.html#how-does-it-work",
    "title": "RoundUp: Finance Tracking App",
    "section": "How does it work?",
    "text": "How does it work?\nRoundUp operates through two platforms 1) Spend Management Platform (SMP) and 2) Investment Platform (IP). The SMP will round up the user’s purchases to the next dollar and the rounded up amount will be transferred to their investment wallet to be utilized on the IP. The IP will then allow the users will then allow the users to invest their micro savings into various funds according to their risk appetite. Based on their risk profile, users will be able to invest in funds and gain the corressponding returns."
  },
  {
    "objectID": "posts/design_portfolio/round_up.html#how-does-it-help-users",
    "href": "posts/design_portfolio/round_up.html#how-does-it-help-users",
    "title": "RoundUp: Finance Tracking App",
    "section": "How does it help users?",
    "text": "How does it help users?\nThe combination of the two platforms allows users to save money without feeling the “pinch” of having to set aside a large amount and be able to invest in funds managed by indestry professionals. They will be able to passively invest while gaining actively managed returns."
  },
  {
    "objectID": "posts/design_portfolio/round_up.html#designs",
    "href": "posts/design_portfolio/round_up.html#designs",
    "title": "RoundUp: Finance Tracking App",
    "section": "Designs",
    "text": "Designs"
  },
  {
    "objectID": "posts/full_stack/mindful.html",
    "href": "posts/full_stack/mindful.html",
    "title": "MindFul: Unconsious Bias",
    "section": "",
    "text": "This project explores global life expectancy trends through interactive data visualizations."
  },
  {
    "objectID": "posts/machine_learning_projects/AgentCore.html#project-goal",
    "href": "posts/machine_learning_projects/AgentCore.html#project-goal",
    "title": "Building a Customer Support Agent Using Amazon BedRock AgentCore",
    "section": "Project Goal",
    "text": "Project Goal\nThis project is part of the AWS Users Group Workshop on 17th September, 2025. The goal of the project is to build a comprehensive, production ready Customer Support Agent using Amazon BedRock AgentCore.\n\nBusiness Scenario\nYou work for TechCorp, an e-commerce company that receives hundreds of customer support requests daily.\nCustomers contact support for various reasons:\n\n\nProduct Information: Getting specifications, pricing, and availability details\n\n\nPolicy Questions: Understanding return policies, shipping costs, and business hours\n\n\nCurrently, your support team spends significant time on repetitive tasks, leading to longer wait times and higher operational costs. You need an AI solution that can handle routine inquiries while escalating complex cases to human agents."
  },
  {
    "objectID": "posts/machine_learning_projects/AgentCore.html#concepts-to-know",
    "href": "posts/machine_learning_projects/AgentCore.html#concepts-to-know",
    "title": "Building a Customer Support Agent Using Amazon BedRock AgentCore",
    "section": "Concepts to know",
    "text": "Concepts to know\n\nAmazon Bedrock AgentCore\nA fully managed service that enables you to deploy and operate highly capable AI agents securely at scale. AgentCore services can be used together or independently and work with any framework including Strands Agents, LangGraph, CrewAI, and LlamaIndex, as well as any foundation model in or outside of Amazon Bedrock, giving you the ultimate flexibility. It serves developers and enterprises who need 1) robust, secure, and scalable infrastructure to support dynamic execution paths at runtime, 2) controls to monitor behavior, 3) powerful tools to enhance agents, and 4) the flexibility to adapt as the landscape evolves.\n\n\nModel Context Protocol (MCP)\nAn open-spourced standard for connecting AI applications to external systems. Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)—enabling them to access key information and perform tasks. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems. Another standardized protocol is called Agent2Agent (A2A).\n\n\nStrands Agent\nStrands Agents is a simple-to-use, code-first framework for building agents. With Strands, developers can simply define a prompt and a list of tools in code to build an agent, then test it locally and deploy it to the cloud. Like the two strands of DNA, Strands connects two core pieces of the agent together: the model and the tools. Strands plans the agent’s next steps and executes tools using the advanced reasoning capabilities of models. For more complex agent use cases, developers can customize their agent’s behavior in Strands. For example, you can specify how tools are selected, customize how context is managed, choose where session state and memory are stored, and build multi-agent applications. Strands can run anywhere and can support any model with reasoning and tool use capabilities, including models in Amazon Bedrock, Anthropic, Ollama, Meta, and other providers through LiteLLM."
  },
  {
    "objectID": "posts/machine_learning_projects/AgentCore.html#architecture-overview",
    "href": "posts/machine_learning_projects/AgentCore.html#architecture-overview",
    "title": "Building a Customer Support Agent Using Amazon BedRock AgentCore",
    "section": "Architecture Overview",
    "text": "Architecture Overview\n\n\n\nArchitecture Diagram"
  },
  {
    "objectID": "posts/machine_learning_projects/AgentCore.html#lab-1",
    "href": "posts/machine_learning_projects/AgentCore.html#lab-1",
    "title": "Building a Customer Support Agent Using Amazon BedRock AgentCore",
    "section": "Lab 1",
    "text": "Lab 1\nIn this lab, we will build a basic Customer Support Agent prototype using Strands Agents. This agent will have the following local tools available: Jupyter Notebook\n\n\n\n\n\nTool Function\n\n\nDescription\n\n\n\n\n\n\nget_return_policy()\n\n\nGet return policy for specific products\n\n\n\n\nget_product_info()\n\n\nGet product information\n\n\n\n\nweb_search()\n\n\nSearch web for updated product information\n\n\n\n\n\nThe main goal of this lab is to:\n\n\nCreate tools using the @tool decorator\n\n\nInitialize a Strands Agent with model and tools\n\n\nTest Agent locally in a Jupyter Notebook\n\n\n\nArchitecture for Lab 1\n\n\n\nLab 1 Architecture Diagram\n\n\n\n\nStep 1: Create Customer Support Tools\nTo provide more capabilities to the Agent, we can build specialized functions that can interact with external systems and data sources. Each tool represents a specific capability that allows the agent to take actions in the real world, from looking up orders to checking policies."
  },
  {
    "objectID": "posts/machine_learning_projects/AgentCore.html#lab-2",
    "href": "posts/machine_learning_projects/AgentCore.html#lab-2",
    "title": "Building a Customer Support Agent Using Amazon BedRock AgentCore",
    "section": "Lab 2",
    "text": "Lab 2\nThis lab focuses on enhancing an agent with memory. The reality for most AI agents today: Every conversation starts from zero, creating:\n\n\nFrustrated customers who must repeat their information repeatedly\n\n\nInefficient support that cannot build on previous interactions\n\n\nLost opportunities to provide personalized, proactive service\n\n\nPoor customer satisfaction due to impersonal, generic responses\n\n\nAmazon BedRock AgentCore Memory addresses this limitation by providing a managed service that enables AI agents to maintain context over time, remember important facts, and deliver consistent, personalized experiences. AgentCore Memory operates on two levels:\n\n\nShort-Term Memory: Immediate conversation context and session-based information that provides continuity within a single interaction or closely related sessions.\n\n\nLong-Term Memory: Persistent information extracted and stored across multiple conversations, including facts, preferences, and summaries that enable personalized experiences over time.\n\n\nIn this lab, you’ll upgrade your Lab 1 prototype to deliver exceptional customer experiences through intelligent memory. Our agent will evolve from a forgetful prototype to a customer-aware assistant that:\n\n\n“Welcome back, Sarah!” - Instantly recognizes returning customers\n\n\n“I remember you prefer email updates” - Recalls individual preferences automatically\n\n\n“Following up on your laptop issue from last month” - Connects related conversations seamlessly\n\n\n“Based on your purchase history, here’s what I recommend” - Provides personalized suggestions\n\n\n\nArchitecture for Lab 2\n\n\n\nLab 2 Architecture Diagram\n\n\n\n\nStep 1: Create AgentCore Memory Resources\nOur first step (This is a one-time setup) is to create a managed agentcore memory resource with multiple strategies (USER_PREFERENCE and SEMANTIC) to store comprehensive customer context, enabling persistent recall across conversations and balanced behavioral/factual insights."
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "",
    "text": "In this Lab, you will learn how to integrate tools available in your organization with the Customer Support Agent using the Amazon Bedrock Gateway.\nThe Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to Large Language Models (LLMs).\nWith Amazon Bedrock Agent Core Gateway, developers can convert APIs, Lambda functions, and existing services into MCP-compatible tools and make them available to agents through Gateway endpoints with just a few lines of code.\nWorkshop Journey:\n\nLab 1 (Done): Create Agent Prototype - Built a functional customer support agent\nLab 2 (Done): Enhance with Memory - Added conversation context and personalization\nLab 3 (Current): Scale with Gateway & Identity - Shared tools across agents securely\nLab 4: Deploy to Production - Used AgentCore Runtime with observability\nLab 5: Build User Interface - Create a customer-facing application\n\n\n\nCurrent State (Lab 1-2): Each agent has its own copy of tools. I practice that is not scalable and leads to:\n\nCode duplication across different agents\nInconsistent tool behavior and maintenance overhead\nNo centralized security or access control\nDifficulty scaling to multiple use cases\n\nAfter this lab, we will have centralized, reusable tools that can serve:\n\nCustomer Support Agent (our current use case)\nSales Agent (needs same product info and customer data)\nInventory Agent (needs same product info and warranty checking)\nReturns Processing Agent (needs return policies and customer profiles)\n\nand other use cases.\n\n\n\nAdditionally, AgentCore Gateway requires you to securely authenticate both inbound and outbound connections. AgentCore Identity provides seamless agent identity and access management across AWS services and third-party applications such as Slack and Zoom while supporting any standard identity providers such as Okta, Entra, and Amazon Cognito. In this lab we will see how AgentCore Gateway integrates with AgentCore Identity to provide secure connections via inbound and outbound authentication.\nFor the inbound authentication, the AgentCore Gateway analyzes the OAuth token passed during invocation to decide allow or deny the access to a tool in the gateway. If a tool needs access to external resources, the AgentCore Gateway can use outbound authentication via API Key, IAM or OAuth Token to allow or deny the access to the external resource.\nDuring the inbound authorization flow, an agent or the MCP client calls an MCP tool in the AgentCore Gateway adding an OAuth access token (generated from the user’s IdP). AgentCore Gateway then validates the OAuth access token and performs inbound authorization.\nIf the tool running in AgentCore Gateway needs to access external resources, OAuth will retrieve credentials of downstream resources using the resource credential provider for the Gateway target. AgentCore Gateway pass the authorization credentials to the caller to get access to the downstream API."
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#overview",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#overview",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "",
    "text": "In this Lab, you will learn how to integrate tools available in your organization with the Customer Support Agent using the Amazon Bedrock Gateway.\nThe Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to Large Language Models (LLMs).\nWith Amazon Bedrock Agent Core Gateway, developers can convert APIs, Lambda functions, and existing services into MCP-compatible tools and make them available to agents through Gateway endpoints with just a few lines of code.\nWorkshop Journey:\n\nLab 1 (Done): Create Agent Prototype - Built a functional customer support agent\nLab 2 (Done): Enhance with Memory - Added conversation context and personalization\nLab 3 (Current): Scale with Gateway & Identity - Shared tools across agents securely\nLab 4: Deploy to Production - Used AgentCore Runtime with observability\nLab 5: Build User Interface - Create a customer-facing application\n\n\n\nCurrent State (Lab 1-2): Each agent has its own copy of tools. I practice that is not scalable and leads to:\n\nCode duplication across different agents\nInconsistent tool behavior and maintenance overhead\nNo centralized security or access control\nDifficulty scaling to multiple use cases\n\nAfter this lab, we will have centralized, reusable tools that can serve:\n\nCustomer Support Agent (our current use case)\nSales Agent (needs same product info and customer data)\nInventory Agent (needs same product info and warranty checking)\nReturns Processing Agent (needs return policies and customer profiles)\n\nand other use cases.\n\n\n\nAdditionally, AgentCore Gateway requires you to securely authenticate both inbound and outbound connections. AgentCore Identity provides seamless agent identity and access management across AWS services and third-party applications such as Slack and Zoom while supporting any standard identity providers such as Okta, Entra, and Amazon Cognito. In this lab we will see how AgentCore Gateway integrates with AgentCore Identity to provide secure connections via inbound and outbound authentication.\nFor the inbound authentication, the AgentCore Gateway analyzes the OAuth token passed during invocation to decide allow or deny the access to a tool in the gateway. If a tool needs access to external resources, the AgentCore Gateway can use outbound authentication via API Key, IAM or OAuth Token to allow or deny the access to the external resource.\nDuring the inbound authorization flow, an agent or the MCP client calls an MCP tool in the AgentCore Gateway adding an OAuth access token (generated from the user’s IdP). AgentCore Gateway then validates the OAuth access token and performs inbound authorization.\nIf the tool running in AgentCore Gateway needs to access external resources, OAuth will retrieve credentials of downstream resources using the resource credential provider for the Gateway target. AgentCore Gateway pass the authorization credentials to the caller to get access to the downstream API."
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#architecture-for-lab-3",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#architecture-for-lab-3",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Architecture for Lab 3",
    "text": "Architecture for Lab 3\n\n&lt;img src=\"images/architecture_lab3_gateway.png\" width=\"75%\"/&gt;\n\nWeb search tool is now centralized in AgentCore Gateway with secure identity-based access control. Multiple agents and use cases can share the same tool securely. We will also reuse the check_warranty() tool built for other applications and add the web_search() tool for use within other applications. get_product_info(), get_return_policy(), and get_technical_support remain as local tools as they are specific to the customer support use case\n\nKey Features\n\nSeamlessly integrate AWS Lambda functions: This example shows how to integrate your Agent with existing AWS Lambda functions to check the warranty of an item and to get the customer profile using Amazon Bedrock AgentCore Gateway.\nSecure your Gateway endpoint with Inbound Auth: Only an Agent providing a valid JWT token can connect to the endpoint to use the tools\nConfigure the Agent to use the MCP endpoint: The Agent gets a valid JWT token and uses it to connect to the MCP endpoint provided by AgentCore Gateway"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#prerequisites",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#prerequisites",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPython 3.12+\nAWS credentials configured\nAnthropic Claude 3.7 enabled on Amazon Bedrock\nComplete Lab 2 Add memory to the Customer Support Agent\nThese resources are created for you within an AWS workshop account\n\nAWS Lambda function\nAWS Lambda Execution IAM Role\nAgentCore Gateway IAM Role\nDynamoDB tables used by the AWS Lambda function.\nCognito User Pool and User Pool Client"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-1-install-and-import-required-libraries",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-1-install-and-import-required-libraries",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 1: Install and import required libraries",
    "text": "Step 1: Install and import required libraries\n\n# Install required packages\n%pip install strands-agents \"boto3&gt;=1.39.15\" strands-agents-tools bedrock_agentcore ddgs -q\n\n\n# Import libraries\nfrom strands import Agent\nfrom strands.models import BedrockModel\nfrom strands.tools.mcp import MCPClient\nimport os\nimport sys\nimport boto3\nimport json\nfrom bedrock_agentcore.identity.auth import requires_access_token\nfrom mcp.client.streamable_http import streamablehttp_client\nimport requests\n\nfrom scripts.utils import get_ssm_parameter, put_ssm_parameter, load_api_spec, get_cognito_client_secret\n\nsts_client = boto3.client('sts')\n\n# Get AWS account details\nREGION = boto3.session.Session().region_name\n\ngateway_client = boto3.client(\n    \"bedrock-agentcore-control\",\n    region_name=REGION,\n)\n\nprint(\"✅ Libraries imported successfully!\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-2-give-our-agent-a-tool-to-access-existing-customer-data",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-2-give-our-agent-a-tool-to-access-existing-customer-data",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 2: Give our agent a tool to access existing customer data",
    "text": "Step 2: Give our agent a tool to access existing customer data\nAgentCore Gateway simplifies agent tool integration in three key ways:\nUniversal MCP Support: Instantly make your tools compatible with any agent framework by exposing them through AgentCore Gateway’s MCP standard\nSimple REST Integration: Transform existing REST services into agent tools by just adding them as AgentCore Gateway targets\nLambda Flexibility: Expose Lambda functions as MCP endpoints that can call any API - demonstrated here with a function that checks warranty status\nAgentCore Gateway populates the Lambda context with the name of the tool to invoke, while the parameters passed to the tool are provided in the Lambda event:\nextended_tool_name = context.client_context.custom[\"bedrockAgentCoreToolName\"]\nresource = extended_tool_name.split(\"___\")[1]\nLambda function\ndef lambda_handler(event, context):\n    if get_tool_name(event) == \"check_warranty_status\":\n        serial_number = get_named_parameter(event=event, name=\"serial_number\")\n        customer_email = get_named_parameter(event=event, name=\"customer_email\")\n\n        warranty_status = check_warranty_status(serial_number, customer_email)\n        return {\"statusCode\": 200, \"body\": warranty_status}"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-3-convert-your-web-search-tool-to-mcp",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-3-convert-your-web-search-tool-to-mcp",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 3 Convert your web search tool to MCP",
    "text": "Step 3 Convert your web search tool to MCP\nNow that we are developing an MCP server using AgentCore Gateway, we can MCP-ify any tools which we think we’ll use for multiple Agents. One of these tools might be a web search tool like we built in Lab1. As a result, we also converted the web search tool from Lab 1 into a Lambda tool within our AgentCore Gateway:\nWeb search Lambda\nfrom ddgs import DDGS\n\n\ndef web_search(keywords: str, region: str = \"us-en\", max_results: int = 5) -&gt; str:\n    \"\"\"Search the web for updated information.\n    \n    Args:\n        keywords (str): The search query keywords.\n        region (str): The search region: wt-wt, us-en, uk-en, ru-ru, etc.\n        max_results (int): The maximum number of results to return.\n        \n    Returns:\n        List of dictionaries with search results.\n    \"\"\"\n    try:\n        results = DDGS().text(keywords, region=region, max_results=max_results)\n        return results if results else \"No results found.\"\n    except Exception as e:\n        return f\"Search error: {str(e)}\"\n\n\nprint(\"✅ Web search tool ready\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-4-create-your-function-definition-metadata",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-4-create-your-function-definition-metadata",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 4 Create your function definition metadata",
    "text": "Step 4 Create your function definition metadata\nLastly, we need to write tool schema which describes the tools implemented by your Lambda function.\nThis file has been already defined in prerequisite/lambda/api_spec.json\n[\n    {\n        \"name\": \"check_warranty_status\",\n        \"description\": \"Check the warranty status of a product using its serial number and optionally verify via email\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"serial_number\": {\n                    \"type\": \"string\"\n                },\n                \"customer_email\": {\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"serial_number\"\n            ]\n        }\n    },\n    {\n        \"name\": \"web_search\",\n        \"description\": \"Search the web for updated information using DuckDuckGo\",\n        \"inputSchema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"keywords\": {\n                    \"type\": \"string\",\n                    \"description\": \"The search query keywords\"\n                },\n                \"region\": {\n                    \"type\": \"string\",\n                    \"description\": \"The search region (e.g., us-en, uk-en, ru-ru)\"\n                },\n                \"max_results\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The maximum number of results to return\"\n                }\n            },\n            \"required\": [\n                \"keywords\"\n            ]\n        }\n    }\n]"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-5.-create-your-agentcore-gateway",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-5.-create-your-agentcore-gateway",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 5. Create your AgentCore Gateway",
    "text": "Step 5. Create your AgentCore Gateway\nNow let’s create the AgentCore Gateway to expose the Lambda function as MCP-compatible endpoint.\nTo validate the callers authorized to invoke our tools we need to configure the Inbound Auth.\nInbound Auth works using OAuth authorization, the standard for MCP servers. With OAuth the client application must authenticate with the OAuth authorizer before using the Gateway. Your client would receive an access token which is used at runtime.\nYou need to specify an OAuth discovery server and client IDs. The Cloudformation provided with the workshop already provisioned the Cognito UserPool and UserPoolClient and it stored the discovery URL and the Client ID in dedicated SSM parameters.\n\ngateway_name = \"customersupport-gw\"\n\nauth_config = {\n    \"customJWTAuthorizer\": {\n        \"allowedClients\": [\n            get_ssm_parameter(\"/app/customersupport/agentcore/machine_client_id\")\n        ],\n        \"discoveryUrl\": get_ssm_parameter(\"/app/customersupport/agentcore/cognito_discovery_url\")\n    }\n}\n\ntry:\n    # create new gateway\n    print(f\"Creating gateway in region {REGION} with name: {gateway_name}\")\n\n    create_response = gateway_client.create_gateway(\n        name=gateway_name,\n        roleArn= get_ssm_parameter(\"/app/customersupport/agentcore/gateway_iam_role\"),\n        protocolType=\"MCP\",\n        authorizerType=\"CUSTOM_JWT\",\n        authorizerConfiguration=auth_config,\n        description=\"Customer Support AgentCore Gateway\",\n    )\n\n    gateway_id = create_response[\"gatewayId\"]\n\n    gateway = {\n        \"id\": gateway_id,\n        \"name\": gateway_name,\n        \"gateway_url\": create_response[\"gatewayUrl\"],\n        \"gateway_arn\": create_response[\"gatewayArn\"],\n    }\n    put_ssm_parameter(\"/app/customersupport/agentcore/gateway_id\", gateway_id)\n\n    print(f\"✅ Gateway created successfully with ID: {gateway_id}\")\n\nexcept Exception as e:\n    # If gateway exists, collect existing gateway ID from SSM\n    existing_gateway_id = get_ssm_parameter(\"/app/customersupport/agentcore/gateway_id\")\n    print(f\"Found existing gateway with ID: {existing_gateway_id}\")\n    \n    # Get existing gateway details\n    gateway_response = gateway_client.get_gateway(gatewayIdentifier=existing_gateway_id)\n    gateway = {\n        \"id\": existing_gateway_id,\n        \"name\": gateway_response[\"name\"],\n        \"gateway_url\": gateway_response[\"gatewayUrl\"],\n        \"gateway_arn\": gateway_response[\"gatewayArn\"],\n    }\n    gateway_id = gateway['id']"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-6.-add-the-lambda-function-target",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-6.-add-the-lambda-function-target",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 6. Add the Lambda function Target",
    "text": "Step 6. Add the Lambda function Target\nNow we will use the previously defined function definitions from prerequisite/lambda/api_spec.json to create a Lambda target within our Agent Gateway. This will define the tools that your gateway will host.\nGateway allows you to attach multiple targets to a Gateway and you can change the targets / tools attached to a gateway at any point. Each target can have its own credential provider, but Gateway becomes a single MCP URL enabling access to all of the relevant tools for an agent across myriad APIs.\n\ndef load_api_spec(file_path: str) -&gt; list:\n    with open(file_path, \"r\") as f:\n        data = json.load(f)\n        \n    if not isinstance(data, list):\n        raise ValueError(\"Expected a list in the JSON file\")\n    return data\n\ntry:\n    api_spec_file = \"./prerequisite/lambda/api_spec.json\"\n\n    # Validate API spec file exists\n    if not os.path.exists(api_spec_file):\n        print(f\"❌ API specification file not found: {api_spec_file}\")\n        sys.exit(1)\n\n    api_spec = load_api_spec(api_spec_file)\n \n    # Use Cognito for Inbound OAuth to our Gateway\n    lambda_target_config = {\n        \"mcp\": {\n            \"lambda\": {\n                \"lambdaArn\": get_ssm_parameter(\"/app/customersupport/agentcore/lambda_arn\"),\n                \"toolSchema\": {\"inlinePayload\": api_spec},\n            }\n        }\n    }\n\n\n    # Create gateway target\n    credential_config = [{\"credentialProviderType\": \"GATEWAY_IAM_ROLE\"}]\n\n    create_target_response = gateway_client.create_gateway_target(\n        gatewayIdentifier=gateway_id,\n        name=\"LambdaUsingSDK\",\n        description=\"Lambda Target using SDK\",\n        targetConfiguration=lambda_target_config,\n        credentialProviderConfigurations=credential_config,\n    )\n\n    print(f\"✅ Gateway target created: {create_target_response['targetId']}\")\n\nexcept Exception as e:\n    print(f\"❌ Error creating gateway target: {str(e)}\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-7-add-our-new-mcp-based-tools-to-our-support-agent",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-7-add-our-new-mcp-based-tools-to-our-support-agent",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 7: Add our new MCP-based tools to our support agent",
    "text": "Step 7: Add our new MCP-based tools to our support agent\nHere we integrate our authentication token from Cognito into an MCPClient from Strands SDK to create an MCP Server object to integrate with our Strands Agent\n\ndef get_token(client_id: str, client_secret: str, scope_string: str, url: str) -&gt; dict:\n    try:\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        data = {\n            \"grant_type\": \"client_credentials\",\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n            \"scope\": scope_string,\n\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.RequestException as err:\n        return {\"error\": str(err)}"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-7.1.-set-up-a-secure-mcp-client-object",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-7.1.-set-up-a-secure-mcp-client-object",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 7.1. Set up a secure MCP client object",
    "text": "Step 7.1. Set up a secure MCP client object\n\ngateway_access_token = get_token(\n    get_ssm_parameter(\"/app/customersupport/agentcore/machine_client_id\"),\n    get_cognito_client_secret(),\n    get_ssm_parameter(\"/app/customersupport/agentcore/cognito_auth_scope\"),\n    get_ssm_parameter(\"/app/customersupport/agentcore/cognito_token_url\"))\n\nprint(f\"Gateway Endpoint - MCP URL: {gateway['gateway_url']}\")\n\n# Set up MCP client\nmcp_client = MCPClient(\n    lambda: streamablehttp_client(\n        gateway['gateway_url'],\n        headers={\"Authorization\": f\"Bearer {gateway_access_token['access_token']}\"},\n    )\n)"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-7.2.-access-tools-in-our-agent-using-our-mcp-client",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-7.2.-access-tools-in-our-agent-using-our-mcp-client",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 7.2. Access tools in our agent using our MCP client",
    "text": "Step 7.2. Access tools in our agent using our MCP client\nNow we will create our Strands Agent using the AgentCore Gateway we built along with the resources from previous labs. Our agent now uses a mix of local tools via our Strands Agent and MCP tools via AgentCore Gateway\n\nfrom lab_helpers.lab1_strands_agent import get_product_info, get_return_policy, get_technical_support, SYSTEM_PROMPT\nfrom lab_helpers.lab2_memory import CustomerSupportMemoryHooks,create_or_get_memory_resource \nimport uuid\nfrom bedrock_agentcore.memory import MemoryClient\n\nmemory_client = MemoryClient(region_name=REGION)\n\nmemory_id = create_or_get_memory_resource()\nSESSION_ID = str(uuid.uuid4())\nCUSTOMER_ID = \"customer_001\"\nmemory_hooks = CustomerSupportMemoryHooks(memory_id, memory_client, CUSTOMER_ID, SESSION_ID)\n\n# Initialize the Bedrock model\nmodel_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\nmodel = BedrockModel(\n    model_id=model_id,\n    temperature=0.3,  # Balanced between creativity and consistency\n    region_name=REGION\n)\n\ntry:\n    mcp_client.start()\nexcept Exception as e:\n    print(f\"Error initializing agent: {str(e)}\")\n\ntools = (\n            [\n                get_product_info,\n                get_return_policy,\n                get_technical_support\n            ]\n            + mcp_client.list_tools_sync()\n        )\n\n# Create the customer support agent\nagent = Agent(\n    model=model,\n    tools=tools,\n    hooks=[memory_hooks],\n    system_prompt=SYSTEM_PROMPT\n)\n\nprint(\"✅ Customer support agent created successfully!\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-8-test-the-agent-with-mcp-tool-access-to-existing-apis",
    "href": "code_files/aws_agentcore/lab-03-agentcore-gateway.html#step-8-test-the-agent-with-mcp-tool-access-to-existing-apis",
    "title": "Lab 3: Securely connect tools to your Agent with AgentCore Gateway",
    "section": "Step 8: Test the agent with MCP tool access to existing APIs”",
    "text": "Step 8: Test the agent with MCP tool access to existing APIs”\nLet’s test our agent with sample queries to ensure all features work correctly.\n\ntest_prompts = [\n    # Warranty Checks\n    \"List all of your tools\",\n    \"I bought an iphone 14 last month. I don't like it because it heats up. How do I solve it?\",\n    \"I have a Gaming Console Pro device , I want to check my warranty status, warranty serial number is MNO33333333.\",\n    \"What are the warranty support guidelines?\",\n    \"How can I fix Lenovo Thinkpad with a blue screen\",\n    \"Tell me detailed information about the technical documentation on installing a new CPU\"\n]\n\n# Function to test the agent\ndef test_agent_responses(agent, prompts):\n    for i, prompt in enumerate(prompts, 1):\n        print(f\"\\nTest Case {i}: {prompt}\")\n        print(\"-\" * 50)\n        try:\n            response = agent(prompt)\n        except Exception as e:\n            print(f\"Error: {str(e)}\")\n        print(\"-\" * 50)\n\n# Run the tests\ntest_agent_responses(agent, test_prompts)\n\nprint(\"\\\\n✅ Basic testing completed!\")\n\n\nCongratulations! 🎉\nYou have successfully completed Lab 3: Securely connect tools to your Agent with AgentCore Gateway\nWhat You Accomplished:\n\nTool Centralization & Reusability:\n\nMigrated web search from local tool to centralized AgentCore Gateway\nIntegrated existing enterprise Lambda functions (warranty check, customer profile)\nCreated a shared tool infrastructure that multiple agent types can access\n\n\n\nEnterprise-Grade Security:\n\nImplemented JWT-based authentication with Cognito integration\nConfigured secure inbound authorization for gateway access\nEstablished identity-based access control for tool usage\n\n\n\nScalable Architecture Foundation:\n\nBuilt reusable tools that serve multiple use cases (customer support, sales, returns processing)\nEliminated code duplication across different agents\nCreated centralized management for tool updates and maintenance\n\n\n\nCurrent Limitations (We’ll fix these next!):\n\nLocal Development Environment - Still running on your laptop, not production-ready\nLimited Observability - No comprehensive monitoring of agent behavior and performance\nManual Scaling - Cannot automatically handle increased load or multiple concurrent users\n\n\n\nNext Up: Lab 4 - Deploying to Production with AgentCore Runtime\nIn Lab 4, you’ll transform your prototype into a production-ready system with:\n\nAgentCore Runtime for scalable agent deployment\nComprehensive observability with metrics, logging, and tracing\nAuto-scaling capabilities to handle real-world traffic\n\n\n\n\nResources\n\nAmazon Bedrock Agent Core Gateway\nStrands Agents Documentation\nOfficial Customer Support Sample"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "",
    "text": "This notebook provides a comprehensive cleanup process for all resources created during the AgentCore End-to-End tutorial."
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#overview",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#overview",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Overview",
    "text": "Overview\nThis cleanup process will remove: - Memory: AgentCore Memory resources and stored data - Runtime: Agent runtime instances and ECR repositories - Security: Execution roles, and Authorization Provider resources - Observability: CloudWatch log groups and streams - Local Files: Generated configuration and code files\n⚠️ Important: This cleanup is irreversible. Make sure you have saved any important data (if needed) before proceeding."
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#step-1-import-required-dependencies",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#step-1-import-required-dependencies",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Step 1: Import Required Dependencies",
    "text": "Step 1: Import Required Dependencies\nLoad all necessary modules and helper functions for the cleanup process.\n\nimport boto3\nimport os\nfrom botocore.exceptions import ClientError\n\nfrom bedrock_agentcore_starter_toolkit import Runtime\nfrom lab_helpers.lab2_memory import delete_memory, REGION\nfrom lab_helpers.utils import (\n    delete_agentcore_runtime_execution_role,\n    delete_ssm_parameter,\n    cleanup_cognito_resources,\n    get_customer_support_secret,\n    delete_customer_support_secret,\n    agentcore_memory_cleanup,\n    gateway_target_cleanup,\n    runtime_resource_cleanup,\n    delete_observability_resources,\n    local_file_cleanup\n)\n\nprint(\"✅ Dependencies imported successfully\")\nprint(f\"🌍 Working in region: {REGION}\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#step-2-clean-up-memory-resources",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#step-2-clean-up-memory-resources",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Step 2: Clean Up Memory Resources",
    "text": "Step 2: Clean Up Memory Resources\nRemove AgentCore Memory resources and associated data.\n\nprint(\"🧠 Starting Memory cleanup...\")\nagentcore_memory_cleanup()"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#step-3-clean-up-runtime-resources",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#step-3-clean-up-runtime-resources",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Step 3: Clean Up Runtime Resources",
    "text": "Step 3: Clean Up Runtime Resources\nRemove the AgentCore Runtime, ECR repository, and associated AWS resources.\n\nprint(\"🚀 Starting Runtime cleanup...\")\nruntime_resource_cleanup()"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#step-4-clean-up-gateway-resources",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#step-4-clean-up-gateway-resources",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Step 4: Clean Up Gateway Resources",
    "text": "Step 4: Clean Up Gateway Resources\nRemove targets, Gateway\n\nprint(\"⚙️ Starting Gateway Cleanup...\")\ngateway_target_cleanup()"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#step-5-clean-up-security-resources",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#step-5-clean-up-security-resources",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Step 5: Clean Up Security Resources",
    "text": "Step 5: Clean Up Security Resources\nRemove execution roles, and authentication resources.\n\nprint(\"🛡️  Starting Security cleanup...\")\nimport json\ntry:\n    # bedrock_client = boto3.client(\"bedrock\", region_name=REGION)\n    \n    # Delete execution role\n    print(\"  🗑️  Deleting AgentCore Runtime execution role...\")\n    delete_agentcore_runtime_execution_role()\n    print(\"  ✅ Execution role deleted\")\n    \n    # Delete SSM parameter\n    print(\"  🗑️  Deleting SSM parameter...\")\n    delete_ssm_parameter(\"/app/customersupport/agentcore/runtime_arn\")\n    print(\"  ✅ SSM parameter deleted\")\n    \n    # Clean up Cognito and secrets\n    print(\"  🗑️  Cleaning up Cognito resources...\")\n    cs = json.loads(get_customer_support_secret())\n    cleanup_cognito_resources(cs['pool_id'])\n    print(\"  ✅ Cognito resources cleaned up\")\n    \n    print(\"  🗑️  Deleting customer support secret...\")\n    delete_customer_support_secret()\n    print(\"  ✅ Customer support secret deleted\")\n    \nexcept Exception as e:\n    print(f\"  ⚠️  Error during security cleanup: {e}\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#step-6-clean-up-local-files",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#step-6-clean-up-local-files",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Step 6: Clean Up Local Files",
    "text": "Step 6: Clean Up Local Files\nRemove generated configuration and code files from the local directory.\n\nprint(\"📁 Starting Local Files cleanup...\")\nlocal_file_cleanup()"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#step-7-clean-up-observability-resources",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#step-7-clean-up-observability-resources",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "Step 7: Clean Up Observability Resources",
    "text": "Step 7: Clean Up Observability Resources\nRemove CloudWatch log groups and streams used for agent monitoring.\n\nprint(\"📊 Starting Observability cleanup...\")\n\ndelete_observability_resources()"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-06-cleanup.html#cleanup-complete",
    "href": "code_files/aws_agentcore/lab-06-cleanup.html#cleanup-complete",
    "title": "🧹 AgentCore End-to-End Cleanup",
    "section": "🎉 Cleanup Complete!",
    "text": "🎉 Cleanup Complete!\nAll AgentCore resources have been cleaned up. Here’s a summary of what was removed:\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"🧹 CLEANUP COMPLETED SUCCESSFULLY! 🧹\")\nprint(\"=\" * 60)\nprint()\nprint(\"📋 Resources cleaned up:\")\nprint(\"  🧠 Memory: AgentCore Memory resources and data\")\nprint(\"  🚀 Runtime: Agent runtime and ECR repository\")\nprint(\"  🛡️ Security: Roles, and SSM secrets\")\nprint(\"  📊 Observability: CloudWatch logs\")\nprint(\"  📁 Files: Local configuration files\")\nprint()\nprint(\"✨ Your AWS account is now clean and ready for new experiments!\")\nprint(\"\\nThank you for completing the AgentCore End-to-End tutorial! 🚀\")"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-05-frontend.html",
    "href": "code_files/aws_agentcore/lab-05-frontend.html",
    "title": "Lab 5: Building a Customer-Facing Frontend Application",
    "section": "",
    "text": "In the previous labs, we’ve built a comprehensive Customer Support Agent with memory, shared tools, and production-grade deployment. With them we show cased the capabilities of AgentCore services to move an agentic use case from prototype to production. You can now invoke your agent runtime from any application. On real world applications, customers expect an user interface to be available. Now it’s time to create a user-friendly frontend that customers can actually use to interact with our agent.\nWorkshop Journey: - Lab 1 (Done): Create Agent Prototype - Built a functional customer support agent - Lab 2 (Done): Enhance with Memory - Added conversation context and personalization - Lab 3 (Done): Scale with Gateway & Identity - Shared tools across agents securely - Lab 4 (Done): Deploy to Production - Used AgentCore Runtime with observability - Lab 5 (Current): Build User Interface - Create a customer-facing application\nIn this lab, we’ll create a Streamlit-based web application that provides customers with an intuitive chat interface to interact with our deployed Customer Support Agent. The frontend will include:\n\nSecure Authentication - User login via Amazon Cognito\nReal-time Chat Interface - Streamlit-powered conversational UI\nStreaming Responses - Live response streaming for better user experience\nSession Management - Persistent conversations with memory\nResponse Timing - Performance metrics for transparency\n\n\n\nOur frontend application connects to the AgentCore Runtime endpoint we deployed in Lab 4, providing a complete end-to-end customer support solution:\n\n&lt;img src=\"images/architecture_lab5_streamlit.png\" width=\"100%\"/&gt;\n\n\n\n\n\nHow to integrate Secure Authentication with a frontend.\nHow to implement real-time streaming responses\nHow to manage user sessions and conversation context\nHow to create an intuitive chat interface for customer support\n\n\n\n\nBy the end of this lab, you will have:\n\nDeployed a customer-facing Streamlit web application\nIntegrated secure user authentication with AgentCore Identity\nImplemented real-time streaming chat responses\nCreated a complete end-to-end customer support solution\nTested the full customer journey from login to support resolution"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-05-frontend.html#overview",
    "href": "code_files/aws_agentcore/lab-05-frontend.html#overview",
    "title": "Lab 5: Building a Customer-Facing Frontend Application",
    "section": "",
    "text": "In the previous labs, we’ve built a comprehensive Customer Support Agent with memory, shared tools, and production-grade deployment. With them we show cased the capabilities of AgentCore services to move an agentic use case from prototype to production. You can now invoke your agent runtime from any application. On real world applications, customers expect an user interface to be available. Now it’s time to create a user-friendly frontend that customers can actually use to interact with our agent.\nWorkshop Journey: - Lab 1 (Done): Create Agent Prototype - Built a functional customer support agent - Lab 2 (Done): Enhance with Memory - Added conversation context and personalization - Lab 3 (Done): Scale with Gateway & Identity - Shared tools across agents securely - Lab 4 (Done): Deploy to Production - Used AgentCore Runtime with observability - Lab 5 (Current): Build User Interface - Create a customer-facing application\nIn this lab, we’ll create a Streamlit-based web application that provides customers with an intuitive chat interface to interact with our deployed Customer Support Agent. The frontend will include:\n\nSecure Authentication - User login via Amazon Cognito\nReal-time Chat Interface - Streamlit-powered conversational UI\nStreaming Responses - Live response streaming for better user experience\nSession Management - Persistent conversations with memory\nResponse Timing - Performance metrics for transparency\n\n\n\nOur frontend application connects to the AgentCore Runtime endpoint we deployed in Lab 4, providing a complete end-to-end customer support solution:\n\n&lt;img src=\"images/architecture_lab5_streamlit.png\" width=\"100%\"/&gt;\n\n\n\n\n\nHow to integrate Secure Authentication with a frontend.\nHow to implement real-time streaming responses\nHow to manage user sessions and conversation context\nHow to create an intuitive chat interface for customer support\n\n\n\n\nBy the end of this lab, you will have:\n\nDeployed a customer-facing Streamlit web application\nIntegrated secure user authentication with AgentCore Identity\nImplemented real-time streaming chat responses\nCreated a complete end-to-end customer support solution\nTested the full customer journey from login to support resolution"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-05-frontend.html#prerequisites",
    "href": "code_files/aws_agentcore/lab-05-frontend.html#prerequisites",
    "title": "Lab 5: Building a Customer-Facing Frontend Application",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCompleted Labs 1-4\nPython 3.10+ installed locally\nStreamlit and required frontend dependencies\nAgentCore Runtime endpoint from Lab 4 (deployed and ready)\nAmazon Cognito user pool configured for authentication\n\n\nStep 1: Install Frontend Dependencies\nFirst, let’s install the required packages for our Streamlit frontend application.\n\n# Install frontend-specific dependencies\n%pip install -r lab_helpers/lab5_frontend/requirements.txt -q\nprint(\"✅ Frontend dependencies installed successfully!\")\n\n\n\nStep 2: Understanding the Frontend Architecture\nOur Streamlit application consists of several key components:\n\nCore Components:\n\nmain.py - Main Streamlit application with UI and authentication\nchat.py - Chat management and AgentCore Runtime integration\nchat_utils.py - Utility functions for message formatting and display\nsagemaker_helper.py - Helper for generating accessible URLs\n\n\n\nAuthentication Flow:\n\nUser accesses the Streamlit application\nAmazon Cognito handles user authentication\nValid JWT tokens are used to authorize AgentCore Runtime requests\nUser can interact with the Customer Support Agent securely\n\n\n\n\nStep 3: Launch the Customer Support Frontend 🚀\nNow let’s start our Streamlit application. The application will:\n\nGenerate an accessible URL for the application\nStart the Streamlit server on port 8501\nConnect to your deployed AgentCore Runtime from Lab 4\nProvide a complete customer support interface\n\nImportant Notes: - The application will run continuously until you stop it (Ctrl+C) - Make sure your AgentCore Runtime from Lab 4 is still deployed and running - The Cognito authentication tokens are valid for 2 hours\n\n# Get the accessible URL for the Streamlit application\nfrom lab_helpers.lab5_frontend.sagemaker_helper import get_streamlit_url\n\nstreamlit_url = get_streamlit_url()\nprint(f'\\n🚀 Customer Support Streamlit Application URL:\\n{streamlit_url}\\n')\n\n# Start the Streamlit application\n!cd lab_helpers/lab5_frontend/ && streamlit run main.py\n\n\n\nStep 4: Testing Your Customer Support Application\nOnce your Streamlit application is running, you can test the complete customer support experience:\n\nAuthentication Testing:\n\nAccess the application using the Customer Support Streamlit Application URL provided above\nSign in with the test credentials provided in the output\nVerify that you see the welcome message with your username\n\n\n&lt;img src=\"images/lab5_streamlit_login.png\"/&gt;\n\n\n&lt;img src=\"images/lab5_welcome_user.png\"/&gt;\n\n\n\nCustomer Support Scenarios to Test:\nProduct Information Queries: “What are the specifications for your laptops?”\nReturn Policy Questions: “What’s the return policy for electronics?”\nTroubleshooting Support: “My iPhone is overheating, what should I do?”\n\n&lt;img src=\"images/lab5_agent_question.png\" width=\"75%\"/&gt;\n\nMemory and Personalization Testing: Have a conversation, then refresh the page\n\n&lt;img src=\"images/lab5_agent_chat_history.png\" width=\"75%\"/&gt;\n\n\n\nWhat to Observe:\n\nReal-time streaming - Responses appear as they’re generated\nResponse timing - Performance metrics displayed with each response\nMemory persistence - Agent remembers conversation context\nTool integration - Agent uses appropriate tools for different queries\nProfessional UI - Clean, intuitive customer support interface\nError handling - Graceful handling of any issues"
  },
  {
    "objectID": "code_files/aws_agentcore/lab-05-frontend.html#lab-5-complete",
    "href": "code_files/aws_agentcore/lab-05-frontend.html#lab-5-complete",
    "title": "Lab 5: Building a Customer-Facing Frontend Application",
    "section": "🎉 Lab 5 Complete!",
    "text": "🎉 Lab 5 Complete!\nCongratulations! You’ve successfully built and deployed a complete customer-facing frontend application for your AI-powered Customer Support Agent. Here’s what you accomplished:\n\nWhat You Built\n\nWeb Interface - Streamlit-based customer support application\nSecure Authentication - Amazon Cognito integration for user management\nReal-time Streaming - Live response streaming for better user experience\nSession Management - Persistent conversations with memory across interactions\nComplete Integration - Frontend connected to your AgentCore Runtime\n\n\n\nEnd-to-End Customer Support Solution\nYou now have a complete, customer support system that includes:\n\nIntelligent Agent (Lab 1) - AI-powered support with custom tools\nPersistent Memory (Lab 2) - Conversation context and personalization\nShared Tools & Identity (Lab 3) - Scalable tool sharing and access control\nProduction Runtime (Lab 4) - Secure, scalable deployment with observability\nCustomer Frontend (Lab 5) - web interface for end users\n\n\n\nKey Capabilities Demonstrated\n\nMulti-turn Conversations - Agent maintains context across interactions\nTool Integration - Seamless use of product info, return policy, and web search\nMemory Persistence - Customer preferences and history maintained\nReal-time Performance - Streaming responses with performance metrics\nSecurity & Identity - Proper authentication and authorization\nObservability - Full tracing and monitoring of agent behavior\n\n\n\nNext Steps\nTo further enhance your customer support solution, consider:\n\nCustom Styling - Brand the frontend with your company’s design system\nAdditional Tools - Integrate with your existing CRM, ticketing, or knowledge base systems\nMulti-language Support - Add internationalization for global customers\nAdvanced Analytics - Implement custom dashboards for support team insights\nMobile Optimization - Ensure the interface works well on mobile devices\n\n\n\nCleanup\nWhen you’re ready to clean up the resources created in this workshop:\nReady to clean up? Proceed to Lab 6: Cleanup →\n\n🎊 Congratulations on completing the Amazon Bedrock AgentCore End-to-End Workshop!\nYou’ve successfully built a complete, production-ready AI agent solution from prototype to customer-facing application using Amazon Bedrock AgentCore capabilities."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Aarushi Nema",
    "section": "",
    "text": "LangChain101\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models and LangChain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Customer Support Agent Using Amazon BedRock AgentCore\n\n\n\nMachine Learning\n\nAgentic AI\n\nAWS\n\n\n\n\n\n\n\n\n\nSep 17, 2025\n\n\nAarushi Nema\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization: A Global Journey Through Life Expectancy\n\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\nAarushi Nema\n\n\n\n\n\n\n\n\n\n\n\n\nMindFul: Unconsious Bias\n\n\n\nApp Development\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nAarushi Nema\n\n\n\n\n\n\n\n\n\n\n\n\nRoundUp: Finance Tracking App\n\n\n\nDesign Portfolio\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nAarushi Nema\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "design_portfolio.html",
    "href": "design_portfolio.html",
    "title": "Design Portfolio",
    "section": "",
    "text": "RoundUp: Finance Tracking App\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nAarushi Nema\n\n\n\n\n\nNo matching items"
  }
]