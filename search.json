[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Aarushi Nema",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBackground:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 31, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Aarushi Nema",
    "section": "About me",
    "text": "About me\nHello, World!  I‚Äôm Aarushi, a final year student @ Nanyang Technological University, Singapore where I am majoring in Data Science and Artificial Intelligence. I am passionate about leveraging Data and AI to solve challenging problems, create meaningful value, and design solutions that leave a lasting impact. I also have a keen interest in creating intuitive UI/UX designs, where functionality meets creativity. I throuoghly emjoy working at the cusp of people and technology.\nMy interest in tech started back in 2012 when I watched a host of sci-fi movies. Fast-forward to today, and I‚Äôve had the privilege of working at a semiconductor company, an automobile company and a student-led IT committee.\nIn my free time, I enjoy drawing üé®, cooking üë©‚Äçüç≥, and working out üèãÔ∏è‚Äç‚ôÄÔ∏è."
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Aarushi Nema",
    "section": " Work Experience:",
    "text": "Work Experience:"
  },
  {
    "objectID": "index.html#tools-of-the-trade",
    "href": "index.html#tools-of-the-trade",
    "title": "Aarushi Nema",
    "section": " Tools of the Trade:",
    "text": "Tools of the Trade:"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Aarushi Nema",
    "section": " Contact",
    "text": "Contact\nWould like to have a chat? Click here to send me an e-mail.\nI am also happy to connect on different social and professional platforms. Click the badges below to see my profile."
  },
  {
    "objectID": "posts/post-with-code/Breast Cancer SVM Classifier .html",
    "href": "posts/post-with-code/Breast Cancer SVM Classifier .html",
    "title": "\n\nBreast Cancer Classifier using Support Vector Machine\n\n",
    "section": "",
    "text": "Breast Cancer Classifier using Support Vector Machine"
  },
  {
    "objectID": "posts/post-with-code/Breast Cancer SVM Classifier .html#background",
    "href": "posts/post-with-code/Breast Cancer SVM Classifier .html#background",
    "title": "\n\nBreast Cancer Classifier using Support Vector Machine\n\n",
    "section": "Background:",
    "text": "Background:\nBreast cancer is the most common cancer among women in the world. It accounts for 25% of all cancer cases and affected over 2.1 Million people in 2015 alone. It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.\nEarly diagnosis significantly increases the chances of survival. The key challenge against its detection is how to classify tumors into malignant (cancerous) or benign(non-cancerous). A tumor is considered malignant if the cells can grow into surrounding tissues or spread to distant areas of the body. A benign tumor does not invade nearby tissue nor spread to other parts of the body the way cancerous tumors can. But benign tumors can be serious if they press on vital structures such as blood vessels or nerves.\nMachine Learning techniques can dramatically improve the level of diagnosis of breast cancer. Research shows that experienced physicians can detect cancer with 79% accuracy, while a 91 %( sometimes up to 97%) accuracy can be achieved using Machine Learning techniques."
  },
  {
    "objectID": "posts/post-with-code/Breast Cancer SVM Classifier .html#project-description",
    "href": "posts/post-with-code/Breast Cancer SVM Classifier .html#project-description",
    "title": "\n\nBreast Cancer Classifier using Support Vector Machine\n\n",
    "section": "Project description",
    "text": "Project description\nGithub link\nIn this project, my task is to classify tumors into malignant (cancerous) or benign (non-cancerous) using features obtained from several cell images.\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe the characteristics of the cell nuclei present in the image.\n\n1. Importing necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Train test split of data\nfrom sklearn.model_selection import train_test_split\n\n#Modelling\nfrom sklearn import svm\n\n#Evaluation\nfrom sklearn.metrics import classification_report\n\n\n\n2. Dataset description\nThere are 700 records and each record has 11 characteristics. The columns/characteristics include 9 predictors, the sample ID, and class of the cell. The fields in each record are:\n\n\n\nField name\nDescription\n\n\n\n\nID\nIdentifier\n\n\nClump\nClumpThickness\n\n\nUnifSize\nUniformity of cell size\n\n\nUnifShape\nUniformity of cell shape\n\n\nMargAdh\nMarginal Adhesion\n\n\nSingEpiSize\nSingle Epithelial Cell Size\n\n\nBareNuc\nBare Nuclei\n\n\nBlandChrom\nBland Chromatin\n\n\nNormNucl\nNormal Nucleoli\n\n\nMit\nMitosis\n\n\nClass\nBenign or malignant\n\n\n\n\n\n3. Importing and analysing dataset\n\ncellDataFrame = pd.read_csv('cell_samples.csv')\ncellDataFrame.head()\n\n\n\n\n\n\n\n\nID\nClump\nUnifSize\nUnifShape\nMargAdh\nSingEpiSize\nBareNuc\nBlandChrom\nNormNucl\nMit\nClass\n\n\n\n\n0\n1000025\n5\n1\n1\n1\n2\n1\n3\n1\n1\n2\n\n\n1\n1002945\n5\n4\n4\n5\n7\n10\n3\n2\n1\n2\n\n\n2\n1015425\n3\n1\n1\n1\n2\n2\n3\n1\n1\n2\n\n\n3\n1016277\n6\n8\n8\n1\n3\n4\n3\n7\n1\n2\n\n\n4\n1017023\n4\n1\n1\n3\n2\n1\n3\n1\n1\n2\n\n\n\n\n\n\n\n\ncellDataFrame.shape\n\n(699, 11)\n\n\n\n#count records under each attribute and check if there are any missing attributes\ncellDataFrame.count()\n\nID             699\nClump          699\nUnifSize       699\nUnifShape      699\nMargAdh        699\nSingEpiSize    699\nBareNuc        699\nBlandChrom     699\nNormNucl       699\nMit            699\nClass          699\ndtype: int64\n\n\n\n#Counting the number of malignant and benign cells in the dataset2\ncellDataFrame['Class'].value_counts()\n\n2    458\n4    241\nName: Class, dtype: int64\n\n\n2 implies Benign and 4 implies Malignant\n\ncellDataFrame.dtypes\n\nID              int64\nClump           int64\nUnifSize        int64\nUnifShape       int64\nMargAdh         int64\nSingEpiSize     int64\nBareNuc        object\nBlandChrom      int64\nNormNucl        int64\nMit             int64\nClass           int64\ndtype: object\n\n\n\n\n4. Split the dataset based on the classes\n\n# combination of over-sampling the minority class and under-sampling the majority class can \n# achieve better classifier performance\n# here the minority class is the malignant cells \nmalignantDataFrame = cellDataFrame[cellDataFrame['Class']==4][0:200]\nmalignantDataFrame.head()\n\n\n\n\n\n\n\n\nID\nClump\nUnifSize\nUnifShape\nMargAdh\nSingEpiSize\nBareNuc\nBlandChrom\nNormNucl\nMit\nClass\n\n\n\n\n5\n1017122\n8\n10\n10\n8\n7\n10\n9\n7\n1\n4\n\n\n12\n1041801\n5\n3\n3\n3\n2\n3\n4\n4\n1\n4\n\n\n14\n1044572\n8\n7\n5\n10\n7\n9\n5\n5\n4\n4\n\n\n15\n1047630\n7\n4\n6\n4\n6\n1\n4\n3\n1\n4\n\n\n18\n1050670\n10\n7\n7\n6\n4\n10\n4\n1\n2\n4\n\n\n\n\n\n\n\n\nbenignDataFrame = cellDataFrame[cellDataFrame['Class']==2][0:200]\nbenignDataFrame.head()\n\n\n\n\n\n\n\n\nID\nClump\nUnifSize\nUnifShape\nMargAdh\nSingEpiSize\nBareNuc\nBlandChrom\nNormNucl\nMit\nClass\n\n\n\n\n0\n1000025\n5\n1\n1\n1\n2\n1\n3\n1\n1\n2\n\n\n1\n1002945\n5\n4\n4\n5\n7\n10\n3\n2\n1\n2\n\n\n2\n1015425\n3\n1\n1\n1\n2\n2\n3\n1\n1\n2\n\n\n3\n1016277\n6\n8\n8\n1\n3\n4\n3\n7\n1\n2\n\n\n4\n1017023\n4\n1\n1\n3\n2\n1\n3\n1\n1\n2\n\n\n\n\n\n\n\n\n\n5. Modify dataset based on requirements\n\n#convert 'BareNuc' from object datatype to int datatype\ncellDataFrame = cellDataFrame[pd.to_numeric(cellDataFrame['BareNuc'], errors='coerce').notnull()]\ncellDataFrame['BareNuc'] = cellDataFrame['BareNuc'].astype('int')\ncellDataFrame.dtypes\n\nID             int64\nClump          int64\nUnifSize       int64\nUnifShape      int64\nMargAdh        int64\nSingEpiSize    int64\nBareNuc        int32\nBlandChrom     int64\nNormNucl       int64\nMit            int64\nClass          int64\ndtype: object\n\n\n\n\n7. Remove unwated columns\nWe will remove columns that won‚Äôt help is in prediction (ID and class)\n\ncellDataFrame.columns\n\nIndex(['ID', 'Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize',\n       'BareNuc', 'BlandChrom', 'NormNucl', 'Mit', 'Class'],\n      dtype='object')\n\n\n\nfeatureDataFrame = cellDataFrame[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize',\n       'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\n\n#convert featureDataFrame into numpy n-dimensional array\n#independent variable\nX=np.asarray(featureDataFrame)\n\n#dependent variable\ny=np.asarray(cellDataFrame['Class'])\n\nX[0:5]\n\narray([[ 5,  1,  1,  1,  2,  1,  3,  1,  1],\n       [ 5,  4,  4,  5,  7, 10,  3,  2,  1],\n       [ 3,  1,  1,  1,  2,  2,  3,  1,  1],\n       [ 6,  8,  8,  1,  3,  4,  3,  7,  1],\n       [ 4,  1,  1,  3,  2,  1,  3,  1,  1]], dtype=int64)\n\n\n\n\n8. Divide the data into train/test set\n\n'''\ncellDataFrame (100) --&gt; Train (80 rows) / Test (20 rows)\n\nTrain(X,y) ## X is a 2D array an y is a 1D array\nTest(X, y)\n'''\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4) #random state is used to generate a random number \n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n(546, 9)\n(137, 9)\n(546,)\n(137,)\n\n\n\n\n9. Modelling\nSVC - Support Vector Classifier - those data points near the hyperplane whose perpendicular distance to the hyperplane  If we sum that distance up of all the points near the hyperplane and maximize it such data points would be called SVC  The SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kerneling.  The mathematical function used for the trandformation is known as kernel function, and can be of different types, such as,\n\nLinear\nPolynomial\nRadial Basis Function (RBF)\nSigmoid\n\nEach of these functions has its characteristics, its pros and cons, and its equations, but as there‚Äôs no easy way of knowing which function performs best with any given dataset, we usually choose different functions and compare the results.\nC- The Regularization parameter - tells the SVM optimization how much you want to avoid misclassifying each training example. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n\nclassifier = svm.SVC(kernel='linear', gamma='auto', C=2)\nclassifier.fit(X_train, y_train)\n\ny_predict = classifier.predict(X_test)\n\n\n\n10. Evaluation\n\nprint(classification_report(y_test, y_predict))\n\n              precision    recall  f1-score   support\n\n           2       1.00      0.94      0.97        90\n           4       0.90      1.00      0.95        47\n\n    accuracy                           0.96       137\n   macro avg       0.95      0.97      0.96       137\nweighted avg       0.97      0.96      0.96       137\n\n\n\nprecision = true_positive/(true_positive + false_positive)\nrecall = true_positive/(true_positive + false_negative) = true_positive/total_actual_positive\nF1: harmonical mean of precision = 2((precision  recall)/(precision + recall))\nsupport: how many instances of the class were there\n\n\nHyperparameter tuning for SVM model - C, gamma, epsilon, kernel\n\n1. GridSearchCV\n\n\n2. RandomizedSearchCV"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]