{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702e41f7",
   "metadata": {},
   "source": [
    "# LangChain101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68741ebf",
   "metadata": {},
   "source": [
    "Fundamental concept of LangChain Revolves around invoking an LLM for a specific input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e4fcb",
   "metadata": {},
   "source": [
    "\n",
    "| Aspect        | **LLM (completion model)**                   | **Chat Model**                                 |\n",
    "| ------------- | -------------------------------------------- | ---------------------------------------------- |\n",
    "| **Core**      | General next-token predictor                 | Still an LLM, but fine-tuned for dialogue      |\n",
    "| **Interface** | Single text prompt → single text output      | List of role-based messages → structured reply |\n",
    "| **API**       | `/v1/completions`                            | `/v1/chat/completions`                         |\n",
    "| **Examples**  | `text-davinci-003`, `gpt-3.5-turbo-instruct` | `gpt-3.5-turbo`, `gpt-4`, `gpt-4o`             |\n",
    "| **Best at**   | Plain text generation, continuation          | Conversations, instructions, multi-turn tasks  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ea442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the LLM Wrapper\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b64c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff292173",
   "metadata": {},
   "source": [
    "## The LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811cfe3",
   "metadata": {},
   "source": [
    "Temperature in an LLM model is a measure of randomness in the output. It ranges from 0 to 1, with 0 for more stability and probable results and 1 for more inconsistent but interesting result. For creative tasks, a temperature between 0.70 and 0.90 offers a balance of reliability and creativity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970adc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo-instruct - for plain text continuation via the /v1/completions endpoint. Best for single-shot tasks, text continuation\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc160b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Monday:\n",
      "Warm up: 10 minute jog\n",
      "Circuit 1:\n",
      "- 20 jumping jacks\n",
      "- 10 push-ups\n",
      "- 20 mountain climbers\n",
      "- 10 burpees\n",
      "- 20 high knees\n",
      "- 20 bicycle crunches\n",
      "Repeat circuit 3 times with minimal rest in between exercises.\n",
      "\n",
      "Tuesday:\n",
      "30 minute outdoor run or bike ride. Alternate between 3 minutes at a moderate pace and 1 minute at a faster pace for a total of 30 minutes.\n",
      "\n",
      "Wednesday:\n",
      "Warm up: 5 minute brisk walk\n",
      "Circuit 2:\n",
      "- 20 step-ups (use a bench or stairs)\n",
      "- 10 tricep dips (use a park bench)\n",
      "- 20 squats\n",
      "- 10 lunges (each leg)\n",
      "- 20 Russian twists (use a water bottle or small weight)\n",
      "- 1 minute plank\n",
      "Repeat circuit 3 times with minimal rest in between exercises.\n",
      "\n",
      "Thursday:\n",
      "45 minute hike or nature walk with some incline intervals. Alternate between walking at a steady pace and increasing the incline for short bursts.\n",
      "\n",
      "Friday:\n",
      "Warm up: 5 minute jog\n",
      "Circuit 3:\n",
      "- 20 jumping lunges\n",
      "- 10 push-ups with side plank rotation (5 each side)\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbb5b4",
   "metadata": {},
   "source": [
    "## The Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f7329",
   "metadata": {},
   "source": [
    "In LangChain, a chain is a wrapper around multiple invidividual components. So we use a chain to combine multiple components in a specific sequence. \n",
    "There are multiple types of chains and the most common one is the LLMChain, which consists of:  (1) PromptTemplate (2) a model (LLM/ChatModel), (3) an optiona output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca244b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GreenHydro Bottles\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"eco-friendly water bottle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abfc73",
   "metadata": {},
   "source": [
    "## The Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262ebb3",
   "metadata": {},
   "source": [
    "In LangChain, the memory is how we store and manage the conversation history between the user and the AI. It keeps the context and cohenrency throughout the interaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67bbf7",
   "metadata": {},
   "source": [
    "Types of chains in LangChain:\n",
    "<br>(1) LLMChain = stateless, single prompt → response.\n",
    "<br>(2) ConversationChain = stateful, keeps history in memory and reuses it across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4370bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Tell me about yourself.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI:  Well, I am an artificial intelligence program designed and created by a team of programmers. I am constantly learning and improving my abilities through algorithms and data analysis. My main purpose is to assist and provide information to users like yourself. I am currently housed in a server located in a data center, but I can also be accessed through various devices such as computers, smartphones, and smart speakers. Is there anything specific you would like to know about me?\n",
      "Human: What can you do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI:  Well, I am an artificial intelligence program designed and created by a team of programmers. I am constantly learning and improving my abilities through algorithms and data analysis. My main purpose is to assist and provide information to users like yourself. I am currently housed in a server located in a data center, but I can also be accessed through various devices such as computers, smartphones, and smart speakers. Is there anything specific you would like to know about me?\n",
      "Human: What can you do?\n",
      "AI:  I have a wide range of capabilities, but some of my main functions include answering questions, performing tasks, and providing recommendations based on data analysis. I can also understand and respond to natural language, making it easier for users to communicate with me. Additionally, I am constantly learning and updating my knowledge base, so my abilities are constantly expanding. Is there something specific you would like me to do for you?\n",
      "Human: How can you help me with data analysis?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I can assist with data analysis by quickly sorting through large amounts of data and identifying patterns and trends. I can also provide visualizations and reports to help you better understand the data. Additionally, I can make predictions and recommendations based on the data, which can be useful for decision making. Is there a specific dataset or analysis you would like me to help with?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory # acts as a buffer to store the conversation history\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True, # will print out extra logs about what’s happening inside the chain\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Tell me about yourself.\")\n",
    "conversation.predict(input=\"What can you do?\")\n",
    "conversation.predict(input=\"How can you help me with data analysis?\")\n",
    "\n",
    "# print(conversation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36791bb",
   "metadata": {},
   "source": [
    "## Deep Lake VectorStore "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c59f9d",
   "metadata": {},
   "source": [
    "Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM Apps. It allows us to do searches and data retrieval on these embeddings and their. It also integrates with LangChain. \n",
    "\n",
    "DeepLake is multimodal (allows storage of different types of files + their vectore representations such as text, images, audio, video etc.) It is serverless so we can create and manage cloud datasets. For easy data loading out of the datalake, DeepLake has a data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b137e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a986e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/2361674661.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
      "/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/2361674661.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
      "/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/2361674661.py:18: LangChainDeprecationWarning: This class is deprecated and will be removed in a future version. You can swap to using the `DeeplakeVectorStore` implementation in `langchain-deeplake`. Please do not submit further PRs to this class.See <https://github.com/activeloopai/langchain-deeplake>\n",
      "  db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:23<00:00, 23.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://aarushinema/langchain_course_from_zero_to_hero', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (2, 1)      str     None   \n",
      " metadata     json      (2, 1)      str     None   \n",
      " embedding  embedding  (2, 1536)  float32   None   \n",
      "    id        text      (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e03babf2-977f-11f0-9104-5e86525bdce2',\n",
       " 'e03badaa-977f-11f0-9104-5e86525bdce2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the LLM and embeddings models\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create our docs\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# create Deep Lake dataset\n",
    "my_activeloop_org_id = \"aarushinema\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e99e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrivalqa chain\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40b06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/209183474.py:13: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# create an agent that uses RetrivalQA chain as a tool\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Use this to answer questions\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4278c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1w/hhs18_1x18l2jwpqfzks4jkw0000gn/T/ipykernel_30187/1676272850.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent.run(\"When was Napoleone born?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the Retrieval QA System to answer this question\n",
      "Action: Retrieval QA System\n",
      "Action Input: \"When was Napoleone born?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\n",
      "Napoleon Bonaparte was born in 15 August 1769.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Napoleon Bonaparte was born in 15 August 1769.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Napoleon Bonaparte was born in 15 August 1769.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"When was Napoleone born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94b2bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://aarushinema/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:24<00:00, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://aarushinema/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['58fbf5dc-9781-11f0-9104-5e86525bdce2',\n",
       " '58fbf73a-9781-11f0-9104-5e86525bdce2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reloading an existing vector store and adding more data\n",
    "# load the existing Deep Lake dataset and specify the embedding function\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# create new documents\n",
    "texts = [\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691490c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the wrapper class for GPT3\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "# create a retreiver from the db\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# instantiate tool that uses the retreiver\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Use this to answer questions\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# create an agent that uses the tool\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dfe1224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a retrieval QA system to answer this question\n",
      "Action: Retrieval QA System\n",
      "Action Input: \"When was Michael Jordan born?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Michael Jordan was born on 17 February 1963.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Michael Jordan was born on 17 February 1963.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"When was Michael Jordan born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158d9a8",
   "metadata": {},
   "source": [
    "## Agents in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98b214",
   "metadata": {},
   "source": [
    "In Langchain, agents are high-level components that use language models (LLMs) to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. There are different types of agents in LangChain:\n",
    "1. The `zero-shot-react-description` agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
    "2. The `react-docstore` agent engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document.\n",
    "3. The `self-ask-with-search` agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool.\n",
    "4. The `conversational-react-description` agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79262dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b70284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tools = Tool(\n",
    "        name=\"Google Search\",\n",
    "        description=\"Use this to search the web\",\n",
    "        func=search.run\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6ec2c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A member of the Democratic Party, he was the first African American president. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008\\xa0... Child\\'s First Name (Type or print) lb. Middle Name. BARACK. HUSSEIN. CERTIFICATE OF LIVE BIRTH. FILE 151. NUMBER le. DEPARTMENT OF HEALTH. 61. 10641. Last Name. Jan 28, 2021 ... Obama\\'s name (particularly his middle name Hussein) was the object of xenophobic innuendo questioning his loyalty (especially in the context of\\xa0... Apr 12, 2017 ... Why is Barack Obama\\'s full name \"Barack Hussein Obama\"? It\\'s what his parents named him … President Barack Obama, First Lady Michelle Obama, and their daughters, Malia, left, and Sasha, right, sit for a family portrait in the Oval Office,\\xa0... Barack Obama ; Barack Hussein Obama II. (1961-08-04) August 4, 1961 (age 64) Honolulu, Hawaii, U.S. · Democratic · Michelle Robinson. \\u200b. ( m. · 1992)\\u200b · Malia\\xa0... Barack Hussein Obama II was born August 4, 1961, in Honolulu, Hawaii, to parents Barack H. Obama, Sr., and Stanley Ann Dunham. Obama Barack H. Obama. Photo: Pete Souza, Obama-Biden Transition Project, licensed by Attribution Share Alike 3.0. Full name: Barack Hussein Obama Born: 4\\xa0... it specifically asks for \"Full former name(s). Obama put “None”, when in fact he went by the name Barry Soetoro, and Barry Obama. It is further believed\\xa0... Aug 16, 2024 ... ... Obama, the former president and first lady, endorsed her, the campaign promoted the video as, “The Obamas call Kamala.” And when Harris and\\xa0...'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d44b",
   "metadata": {},
   "source": [
    "We can use the \"k\" parameter to set number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d38bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper(k=1)\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"I'm feeling lucky\",\n",
    "    description=\"Search Google and return the first result\",\n",
    "    func=search.run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80c96813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beda8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "\n",
    "def top5_results(query):\n",
    "    return search.results(query, 5)\n",
    "\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search Snippets\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top5_results,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
